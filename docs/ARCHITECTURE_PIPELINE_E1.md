# üîÑ Architecture Pipeline DataSens E1

**Version** : Align√©e avec `notebooks/datasens_E1_v2.ipynb`

---

## üìã Architecture compl√®te du pipeline

### üèóÔ∏è Stack technique

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              NOTEBOOKS JUPYTER (Low-code)               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  01_setup_env.ipynb      ‚Üí Environnement + Git          ‚îÇ
‚îÇ  02_schema_create.ipynb  ‚Üí DDL PostgreSQL 18 tables     ‚îÇ
‚îÇ  03_ingest_sources.ipynb ‚Üí Ingestion 5 sources          ‚îÇ
‚îÇ  04_crud_tests.ipynb     ‚Üí Tests CRUD + QA               ‚îÇ
‚îÇ  05_snapshot_and_readme.ipynb ‚Üí Bilan + exports          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              INGESTION MULTI-SOURCES                     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  üìÑ Fichier plat CSV    ‚Üí Kaggle (50% PG + 50% MinIO)   ‚îÇ
‚îÇ  üóÑÔ∏è Base de donn√©es     ‚Üí SQLite Kaggle ‚Üí PostgreSQL    ‚îÇ
‚îÇ  üåê API                 ‚Üí OpenWeatherMap, NewsAPI, RSS   ‚îÇ
‚îÇ  üï∑Ô∏è Web Scraping        ‚Üí MonAvisCitoyen (dry-run)      ‚îÇ
‚îÇ  üåç Big Data            ‚Üí GDELT GKG (√©chantillon)       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              STOCKAGE DUAL LAYER                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚òÅÔ∏è MinIO DataLake      ‚Üí Fichiers bruts S3-compatible  ‚îÇ
‚îÇ  üóÑÔ∏è PostgreSQL SGBD    ‚Üí Donn√©es structur√©es Merise     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              TRA√áABILIT√â & GOUVERNANCE                    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  üìÑ Manifests JSON      ‚Üí data/raw/manifests/            ‚îÇ
‚îÇ  üìù Logs structur√©s    ‚Üí logs/collecte_*.log             ‚îÇ
‚îÇ  üîó Flux PostgreSQL     ‚Üí Table flux (id_source, date)    ‚îÇ
‚îÇ  üè∑Ô∏è Git tags            ‚Üí E1_REAL_YYYYMMDD              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üîß Fonctions helpers centralis√©es

### Utilitaires g√©n√©rales

```python
def ts() -> str:
    """Timestamp UTC ISO compact (YYYYMMDDTHHMMSSZ)"""
    return datetime.now(timezone.utc).strftime("%Y%m%dT%H%M%SZ")

def sha256(s: str) -> str:
    """Hash SHA-256 pour d√©duplication"""
    return hashlib.sha256(s.encode("utf-8")).hexdigest()
```

### MinIO DataLake

```python
def ensure_bucket(bucket: str = MINIO_BUCKET):
    """Cr√©e le bucket MinIO s'il n'existe pas"""
    if not minio_client.bucket_exists(bucket):
        minio_client.make_bucket(bucket)

def minio_upload(local_path: Path, dest_key: str) -> str:
    """Upload fichier vers MinIO et retourne URI S3"""
    ensure_bucket(MINIO_BUCKET)
    minio_client.fput_object(MINIO_BUCKET, dest_key, str(local_path))
    return f"s3://{MINIO_BUCKET}/{dest_key}"
```

### PostgreSQL helpers

```python
def get_source_id(conn, nom: str) -> int:
    """R√©cup√®re l'id_source depuis le nom (avec logging)"""
    logger.info(f"[get_source_id] Recherche source: {nom}")
    result = conn.execute(text("SELECT id_source FROM source WHERE nom = :nom"), {"nom": nom}).fetchone()
    return result[0] if result else None

def create_flux(conn, id_source: int, format_type: str = "csv", manifest_uri: str = None) -> int:
    """Cr√©e un flux et retourne id_flux (tra√ßabilit√©)"""
    logger.info(f"[create_flux] Cr√©ation flux pour id_source={id_source}")
    result = conn.execute(text("""
        INSERT INTO flux (id_source, format, manifest_uri)
        VALUES (:id_source, :format, :manifest_uri)
        RETURNING id_flux
    """), {"id_source": id_source, "format": format_type, "manifest_uri": manifest_uri})
    return result.scalar()

def ensure_territoire(conn, ville: str, code_insee: str = None, lat: float = None, lon: float = None) -> int:
    """Cr√©e ou r√©cup√®re un territoire"""
    result = conn.execute(text("SELECT id_territoire FROM territoire WHERE ville = :ville"), {"ville": ville}).fetchone()
    if result:
        return result[0]
    result = conn.execute(text("""
        INSERT INTO territoire (ville, code_insee, lat, lon)
        VALUES (:ville, :code_insee, :lat, :lon)
        RETURNING id_territoire
    """), {"ville": ville, "code_insee": code_insee, "lat": lat, "lon": lon})
    return result.scalar()

def insert_documents(conn, docs: list) -> int:
    """Insertion batch avec gestion doublons (ON CONFLICT)"""
    logger.info(f"[insert_documents] Insertion de {len(docs)} documents...")
    inserted = 0
    for doc in docs:
        try:
            result = conn.execute(text("""
                INSERT INTO document (id_flux, id_territoire, titre, texte, langue, date_publication, hash_fingerprint)
                VALUES (:id_flux, :id_territoire, :titre, :texte, :langue, :date_publication, :hash_fingerprint)
                ON CONFLICT (hash_fingerprint) DO NOTHING
                RETURNING id_doc
            """), doc)
            if result.scalar():
                inserted += 1
        except Exception as e:
            log_error("insert_documents", e, "Erreur insertion")
    return inserted
```

### Logging

```python
def log_error(source: str, error: Exception, context: str = ""):
    """Log une erreur avec traceback complet"""
    error_msg = f"[{source}] {context}: {error!s}"
    logger.error(error_msg)
    logger.error(f"Traceback:\n{traceback.format_exc()}")
```

---

## üìä Pattern d'ingestion standard

Pour chaque source, le pattern est :

```python
logger.info("üìÑ SOURCE X/5 : [Nom source]")
logger.info("=" * 80)

# 1. Collecte
data = collect_from_source()  # API, scraping, CSV...
df = pd.DataFrame(data)

# 2. Pr√©paration (hash, nettoyage)
df["hash_fingerprint"] = df.apply(lambda x: sha256(...), axis=1)
df = df.drop_duplicates(subset=["hash_fingerprint"])

# 3. Sauvegarde locale
local_path = RAW_DIR / "source" / f"source_{ts()}.csv"
df.to_csv(local_path, index=False)

# 4. Upload MinIO (DataLake)
minio_uri = minio_upload(local_path, f"source/{local_path.name}")
logger.info(f"‚òÅÔ∏è MinIO : {minio_uri}")

# 5. Insertion PostgreSQL
with engine.begin() as conn:
    id_source = get_source_id(conn, "Source Name")
    if not id_source:
        # Cr√©er la source si elle n'existe pas
        ...
    id_flux = create_flux(conn, id_source, "csv", minio_uri)

    docs = [prepare_document(row, id_flux) for _, row in df.iterrows()]
    inserted = insert_documents(conn, docs)

logger.info(f"‚úÖ {inserted} documents ins√©r√©s")
```

---

## üîÑ Architecture hybride 50/50 (Kaggle)

**Pattern sp√©cial pour fichier plat** :

```python
# Split 50/50
mid_point = len(df_kaggle) // 2
df_pg = df_kaggle.iloc[:mid_point].copy()  # 50% ‚Üí PostgreSQL
df_raw = df_kaggle.iloc[mid_point:].copy()  # 50% ‚Üí MinIO

# Upload 50% vers MinIO
raw_output = RAW_DIR / "kaggle" / f"kaggle_raw_{ts()}.csv"
df_raw.to_csv(raw_output, index=False)
minio_uri = minio_upload(raw_output, f"kaggle/{raw_output.name}")

# Insertion 50% dans PostgreSQL
with engine.begin() as conn:
    id_flux = create_flux(conn, id_source, "csv", minio_uri)
    docs = [prepare_doc(row, id_flux) for _, row in df_pg.iterrows()]
    inserted = insert_documents(conn, docs)
```

---

## üìã Manifest JSON

**Structure standard** :

```json
{
  "run_id": "20251029T123337Z",
  "timestamp_utc": "2025-10-29T12:33:37Z",
  "notebook_version": "03_ingest_sources.ipynb",
  "sources_ingested": [
    "Kaggle CSV (fichier plat)",
    "Kaggle DB (base de donn√©es)",
    "OpenWeatherMap (API)",
    "MonAvisCitoyen (scraping)",
    "GDELT GKG (big data)"
  ],
  "counts": {
    "documents": 25000,
    "flux": 5,
    "sources": 5,
    "meteo": 20,
    "evenements": 57
  },
  "postgres_db": "datasens",
  "minio_bucket": "datasens-raw",
  "raw_data_location": "data/raw",
  "log_file": "logs/collecte_20251029_124242.log"
}
```

**Stockage** :
- Local : `data/raw/manifests/manifest_*.json`
- MinIO : `s3://datasens-raw/manifests/manifest_*.json`

---

## ‚úÖ Checklist conformit√© pipeline

- [x] Syst√®me de logging structur√© (logs/ + errors/)
- [x] MinIO DataLake int√©gr√© (upload automatique)
- [x] Fonctions helpers centralis√©es r√©utilisables
- [x] Architecture hybride 50/50 (PostgreSQL + MinIO)
- [x] D√©duplication SHA-256
- [x] Tra√ßabilit√© compl√®te (flux, manifests, logs)
- [x] RGPD compliant (hash auteurs, pas de donn√©es personnelles)

---

**üìù Note** : Ce document d√©crit l'architecture pipeline conforme au notebook `datasens_E1_v2.ipynb`.
