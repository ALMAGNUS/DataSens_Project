# üìä WEB SCRAPING MULTI-SOURCES - Documentation D√©taill√©e

## üéØ **Vue d'ensemble**

Le syst√®me collecte des donn√©es citoyennes depuis **5 sources diversifi√©es** pour maximiser la couverture du sentiment public fran√ßais.

---

## üì± **SOURCE 1 : Reddit France (API PRAW)**

### **Configuration requise** :
```properties
REDDIT_CLIENT_ID=ABC123def456ghi
REDDIT_CLIENT_SECRET=xyz789abc123def456ghi789
```

### **Comment obtenir** :
1. Cr√©er compte sur https://www.reddit.com
2. Cr√©er app sur https://www.reddit.com/prefs/apps (type: script)
3. Copier Client ID (14 caract√®res) + Secret (27 caract√®res)
4. Voir `REDDIT_API_SETUP.md` pour guide d√©taill√©

### **Donn√©es collect√©es** :
- **Subreddits** : r/france, r/French, r/AskFrance
- **M√©thode** : Hot posts (tri par popularit√©)
- **Volume** : 150 posts (50 par subreddit)
- **Champs** :
  - `titre` : Titre du post
  - `texte` : Contenu (selftext)
  - `source_site` : reddit.com/r/{subreddit}
  - `url` : Lien permanent
  - `score` : Upvotes - Downvotes
  - `date_publication` : Timestamp cr√©ation

### **Avantages** :
- ‚úÖ **API officielle** (100% l√©gal)
- ‚úÖ **60 requ√™tes/minute** (pas de quota journalier)
- ‚úÖ **Sentiment authentique** (discussions citoyennes brutes)
- ‚úÖ **Fran√ßais natif** (pas de traduction)
- ‚úÖ **Score popularit√©** (upvotes = indicateur engagement)

### **Code technique** :
```python
import praw

reddit = praw.Reddit(
    client_id=REDDIT_CLIENT_ID,
    client_secret=REDDIT_CLIENT_SECRET,
    user_agent="datasens/1.0 (educational project)"
)

for post in reddit.subreddit("france").hot(limit=50):
    data = {
        "titre": post.title,
        "texte": post.selftext,
        "score": post.score,
        "date_publication": pd.to_datetime(post.created_utc, unit='s')
    }
```

---

## üì∫ **SOURCE 2 : YouTube Comments (API Google)**

### **Configuration requise** :
```properties
YOUTUBE_API_KEY=AIzaSyCvLuuu3Z3Ex35FfxKjrRjcgs2_d3UFCWM  # D√âJ√Ä CONFIGUR√â ‚úÖ
```

### **Donn√©es collect√©es** :
- **Cha√Ænes** : France 24, LCI
- **Vid√©os** : 3 plus r√©centes par cha√Æne
- **Volume** : ~300 commentaires (50 par vid√©o)
- **Champs** :
  - `titre` : Extrait commentaire (100 premiers chars)
  - `texte` : Commentaire complet
  - `source_site` : youtube.com/{cha√Æne}
  - `url` : Lien vid√©o
  - `score` : Nombre de likes
  - `date_publication` : Timestamp publication

### **Avantages** :
- ‚úÖ **D√©j√† configur√©** (vous avez la cl√© !)
- ‚úÖ **R√©actions temps r√©el** (actualit√©s chaudes)
- ‚úÖ **Sentiment fort** (opinions tranch√©es)
- ‚úÖ **Quota g√©n√©reux** (10,000 unit√©s/jour)

### **Code technique** :
```python
from googleapiclient.discovery import build

youtube = build('youtube', 'v3', developerKey=YOUTUBE_API_KEY)

# R√©cup√©rer vid√©os r√©centes
search_response = youtube.search().list(
    part="id",
    channelId="UCCCPCZNChQdGa9EkATeye4g",  # France 24
    maxResults=3,
    order="date"
).execute()

# R√©cup√©rer commentaires
comments_response = youtube.commentThreads().list(
    part="snippet",
    videoId=video_id,
    maxResults=50
).execute()
```

---

## üá´üá∑ **SOURCE 3 : SignalConso (Open Data Gouvernemental)**

### **Configuration requise** :
```properties
# AUCUNE ! API publique ouverte ‚úÖ
```

### **Donn√©es collect√©es** :
- **Source** : https://signal.conso.gouv.fr
- **Type** : Signalements consommateurs officiels
- **Volume** : 500 signalements
- **Champs** :
  - `titre` : Cat√©gorie du signalement
  - `texte` : Description d√©taill√©e
  - `source_site` : signal.conso.gouv.fr
  - `date_publication` : Date cr√©ation

### **Avantages** :
- ‚úÖ **Donn√©es gouvernementales** (fiabilit√© maximale)
- ‚úÖ **Aucune API key** (acc√®s libre)
- ‚úÖ **Signalements v√©rifi√©s** (qualit√© contr√¥l√©e)
- ‚úÖ **RGPD compliant** (d√©j√† anonymis√©)
- ‚úÖ **Sentiment citoyen officiel** (plaintes r√©elles)

### **Code technique** :
```python
signal_url = "https://signal.conso.gouv.fr/api/reports"
params = {"limit": 500, "offset": 0}

r = requests.get(signal_url, params=params, timeout=15)
reports = r.json()

for report in reports:
    data = {
        "titre": report.get("category"),
        "texte": report.get("description"),
        "date_publication": pd.to_datetime(report.get("creationDate"))
    }
```

---

## ‚≠ê **SOURCE 4 : Trustpilot France (Web Scraping)**

### **Configuration requise** :
```properties
# AUCUNE ! Scraping mod√©r√© avec respect robots.txt
```

### **Donn√©es collect√©es** :
- **URL** : https://fr.trustpilot.com/categories/public_local_services
- **Type** : Avis sur services publics
- **Volume** : ~100 avis
- **Champs** :
  - `titre` : Titre de l'avis
  - `texte` : Contenu complet
  - `source_site` : trustpilot.com

### **Avantages** :
- ‚úÖ **Notes + texte** (double signal)
- ‚úÖ **Avis v√©rifi√©s** (authentification Trustpilot)
- ‚úÖ **Respect robots.txt** (v√©rifi√© avant scraping)
- ‚úÖ **Rate limiting** (2 sec entre requ√™tes)

### **Code technique** :
```python
from bs4 import BeautifulSoup

# V√©rifier robots.txt
robots_r = requests.get("https://fr.trustpilot.com/robots.txt")
if "Disallow: /categories" not in robots_r.text:
    
    r = requests.get(trust_url, headers={
        "User-Agent": "DataSensBot/1.0 (educational)"
    })
    
    soup = BeautifulSoup(r.text, "html.parser")
    reviews = soup.select(".review-card")
    
    for review in reviews:
        text_el = review.select_one(".review-content__text")
        data = {"texte": text_el.get_text(strip=True)}
```

---

## üß™ **SOURCE 5 : MonAvisCitoyen (Dry-run Test)**

### **Configuration requise** :
```properties
# AUCUNE ! Test simul√©
```

### **Donn√©es collect√©es** :
- **Volume** : 0 (simulation)
- **Usage** : Validation code scraping

### **Avantages** :
- ‚úÖ **Aucun risque** (pas de requ√™tes r√©elles)
- ‚úÖ **D√©mo technique** (robots.txt check, rate limiting)
- ‚úÖ **Fallback** (si autres sources √©chouent)

---

## üìä **VOLUM√âTRIE ATTENDUE**

| Source | Documents | Type |
|--------|-----------|------|
| Reddit France | 150 | Posts discussions |
| YouTube | 300 | Commentaires vid√©os |
| SignalConso | 500 | Signalements officiels |
| Trustpilot | 100 | Avis services publics |
| MonAvisCitoyen | 0 | Test (dry-run) |
| **TOTAL** | **~1050** | Documents citoyens |

**Note** : Volume peut varier selon disponibilit√© APIs et contenu du jour.

---

## üîí **S√âCURIT√â & CONFORMIT√â**

### **RGPD** :
- ‚úÖ Anonymisation automatique (pas de donn√©es personnelles stock√©es)
- ‚úÖ Sources publiques uniquement
- ‚úÖ Respect ToS de chaque plateforme

### **Rate Limiting** :
```python
# Reddit : 60 req/min (g√©r√© par PRAW)
time.sleep(2)  # Entre subreddits

# YouTube : 10,000 unit√©s/jour
time.sleep(1)  # Entre vid√©os

# Trustpilot : Custom
time.sleep(2)  # Entre pages
```

### **robots.txt** :
- ‚úÖ V√©rification syst√©matique avant scraping
- ‚úÖ Respect des Disallow
- ‚úÖ User-Agent identifiable

---

## üöÄ **INSTRUCTIONS D'EX√âCUTION**

### **Pr√©requis** :
1. ‚úÖ Installer d√©pendances : `pip install praw google-api-python-client`
2. ‚úÖ Configurer Reddit API (voir `REDDIT_API_SETUP.md`)
3. ‚úÖ YouTube API d√©j√† configur√©e

### **Ex√©cution** :
1. Ouvrir `notebooks/datasens_E1_v2.ipynb`
2. Ex√©cuter cellule 18 (Web Scraping Multi-Sources)
3. Dur√©e : ~2-3 minutes
4. R√©sultat : ~1050 documents en PostgreSQL + MinIO

### **Optionnel** :
- Reddit peut √™tre skipp√© (autres sources fonctionneront)
- Si erreur : v√©rifier credentials `.env`
- Logs d√©taill√©s affich√©s pour chaque source

---

## üìà **AM√âLIORATIONS FUTURES (E2/E3)**

- [ ] Ajouter Twitter/X via nitter
- [ ] Commentaires Facebook Pages publiques
- [ ] Forums jeuxvideo.com, Dealabs
- [ ] Scraping asynchrone (asyncio)
- [ ] D√©tection langue automatique
- [ ] Sentiment analysis en temps r√©el

---

**Questions ?** Consultez `REDDIT_API_SETUP.md` pour Reddit ou demandez de l'aide ! ü§ù
