```text
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                              â•‘
â•‘                                                                              â•‘
â•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—     â•‘
â•‘     â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•— â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•     â•‘
â•‘     â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—     â•‘
â•‘     â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘ â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â•šâ•â•â•â•â–ˆâ–ˆâ•‘     â•‘
â•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â• â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘     â•‘
â•‘     â•šâ•â•â•â•â•â•  â•šâ•â•  â•šâ•â•   â•šâ•â•   â•šâ•â•  â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•â•â•šâ•â•â•â•â•â•â•     â•‘
â•‘                                                                              â•‘
â•‘                                                                              â•‘
â•‘                   â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—                     â•‘
â•‘                   â•‘   GUIDE TECHNIQUE COMPLET - E1     â•‘                     â•‘
â•‘                   â•‘        Structure 5 Notebooks       â•‘                     â•‘
â•‘                   â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                     â•‘
â•‘                                                                              â•‘
â•‘                      ğŸ“Š Pipeline ETL Multi-Sources                           â•‘
â•‘                      ğŸ—„ï¸ PostgreSQL Merise (18 tables)                        â•‘
â•‘                      ğŸ’¾ MinIO DataLake + Logging                             â•‘
â•‘                      ğŸ“ˆ CRUD + Visualisations + QualitÃ©                      â•‘
â•‘                                                                              â•‘
â•‘                      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                       â•‘
â•‘                           Projet Certifiant 2025                             â•‘
â•‘                      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                       â•‘
â•‘                                                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

# ğŸš€ Guide Technique DataSens E1 - Structure 5 Notebooks

> **Approche pÃ©dagogique** : 5 notebooks Jupyter structurÃ©s, code inline simple et transparent. Architecture de pipeline alignÃ©e sur `datasens_E1_v2.ipynb`. Tout visible pour le jury ! ğŸ’ª

---

## ğŸ“¦ Table des MatiÃ¨res

1. [Vue d'ensemble du projet](#vue-densemble)
2. [Architecture du projet - 5 Notebooks](#architecture-5-notebooks)
3. [Stack technique et dÃ©pendances](#stack-technique)
4. [DÃ©taillÃ© notebook par notebook](#notebook-par-notebook)
5. [Architecture Pipeline (RÃ©fÃ©rence datasens_E1_v2.ipynb)](#architecture-pipeline)
6. [Diagrammes et visualisations](#diagrammes)
7. [Troubleshooting](#troubleshooting)
8. [Glossaire et concepts](#glossaire)

---

## ğŸ¯ Vue d'ensemble du projet {#vue-densemble}

### DataSens E1 : Socle de donnÃ©es pour entraÃ®nement IA

**Objectif principal** : CrÃ©er un jeu de donnÃ©es prÃ©-annotÃ© pour optimiser l'entraÃ®nement de modÃ¨les IA, minimiser la dÃ©rive et les hallucinations.

**ğŸ“ Approche acadÃ©mique** :
- âœ… **5 notebooks Jupyter** structurÃ©s cellule par cellule
- âœ… Code **simple et lisible** dans les cellules (pas de modules `.py` cachÃ©s)
- âœ… **Merise modÃ©lisation** : MCD â†’ MLD â†’ MPD ciblÃ©
- âœ… **5 types de sources** rÃ©ellement ingÃ©rÃ©s (fichier plat, BDD, API, scraping, big data)
- âœ… **CRUD complet** dÃ©montrÃ© dans un notebook dÃ©diÃ©
- âœ… **TraÃ§abilitÃ©** : flux, manifests JSON, logs structurÃ©s, Git versioning

**Le but** : DÃ©montrer au jury qu'on maÃ®trise la collecte multi-sources, le modÃ¨le relationnel Merise, et la gouvernance des donnÃ©es avec du code propre et comprÃ©hensible.

---

## ğŸ—ï¸ Architecture du projet - 5 Notebooks {#architecture-5-notebooks}

### Structure modulaire progressive

```
ğŸ“‚ notebooks/
â”œâ”€â”€ ğŸ““ 01_setup_env.ipynb           â†’ Environnement & connexions
â”œâ”€â”€ ğŸ““ 02_schema_create.ipynb        â†’ DDL PostgreSQL (18 tables Merise)
â”œâ”€â”€ ğŸ““ 03_ingest_sources.ipynb       â†’ Ingestion 5 sources + manifests
â”œâ”€â”€ ğŸ““ 04_crud_tests.ipynb           â†’ DÃ©monstration CRUD complet
â””â”€â”€ ğŸ““ 05_snapshot_and_readme.ipynb â†’ Bilan E1 + exports + roadmap E2/E3
```

### Flux d'exÃ©cution

```
01_setup_env.ipynb
    â†“ (VÃ©rifie environnement, connexions PG/MinIO)
02_schema_create.ipynb
    â†“ (CrÃ©e 18 tables, rÃ©fÃ©rentiels, index)
03_ingest_sources.ipynb
    â†“ (IngÃ¨re 5 sources, crÃ©e manifests, logs)
04_crud_tests.ipynb
    â†“ (DÃ©montre CREATE/READ/UPDATE/DELETE)
05_snapshot_and_readme.ipynb
    â†“ (Exporte DDL, CSV, crÃ©e tag Git)
```

### ğŸ“Š Tableau rÃ©capitulatif des notebooks

| Notebook | Objectif | DurÃ©e estimÃ©e | DÃ©pendances |
|----------|----------|---------------|-------------|
| **01_setup_env** | VÃ©rifier environnement Python, connexions PostgreSQL/MinIO, Git init | 5 min | Python 3.11+, Docker |
| **02_schema_create** | CrÃ©er schÃ©ma PostgreSQL (18 tables), index, rÃ©fÃ©rentiels | 10 min | 01_setup_env exÃ©cutÃ© |
| **03_ingest_sources** | IngÃ©rer 5 sources (Kaggle, OWM, GDELT, etc.) | 30-60 min | 02_schema_create, API keys |
| **04_crud_tests** | DÃ©montrer CRUD complet + qualitÃ© | 15 min | 03_ingest_sources exÃ©cutÃ© |
| **05_snapshot_and_readme** | Exporter DDL/CSV, crÃ©er tag Git, roadmap | 5 min | Tous les notebooks prÃ©cÃ©dents |

---

## ğŸ“‹ Stack technique et dÃ©pendances {#stack-technique}

### ğŸ Python 3.11+ (obligatoire)

**Pourquoi Python 3.11+ ?**
- Performances amÃ©liorÃ©es vs 3.9/3.10
- CompatibilitÃ© avec toutes les libs modernes
- Support natif des annotations de types

### ğŸ“¦ DÃ©pendances principales

#### CatÃ©gorie 1ï¸âƒ£ : Base de donnÃ©es

| Package | Version | Usage |
|---------|---------|-------|
| `sqlalchemy` | >= 2.0 | ORM Python â†’ PostgreSQL |
| `psycopg2-binary` | >= 2.9 | Driver PostgreSQL |
| `python-dotenv` | >= 1.0 | Variables d'environnement `.env` |

**Exemple concret** :
```python
from sqlalchemy import create_engine, text
from dotenv import load_dotenv
load_dotenv()
engine = create_engine(f"postgresql+psycopg2://{user}:{pass}@{host}/{db}")
```

#### CatÃ©gorie 2ï¸âƒ£ : Data processing

| Package | Version | Usage |
|---------|---------|-------|
| `pandas` | >= 2.0 | DataFrames (manipulation donnÃ©es) |
| `numpy` | >= 1.24 | Calculs numÃ©riques |

#### CatÃ©gorie 3ï¸âƒ£ : Collecte donnÃ©es

| Package | Version | Usage |
|---------|---------|-------|
| `requests` | >= 2.31 | HTTP client (APIs REST) |
| `feedparser` | >= 6.0 | Parse RSS/Atom feeds |
| `beautifulsoup4` | >= 4.12 | HTML parsing (web scraping) |
| `selectolax` | >= 0.3 | Alternative rapide Ã  BeautifulSoup |
| `kaggle` | >= 1.5 | API Kaggle datasets |

#### CatÃ©gorie 4ï¸âƒ£ : Storage & Logging

| Package | Version | Usage |
|---------|---------|-------|
| `minio` | >= 7.0 | Client S3 (DataLake) |
| `python-dotenv` | >= 1.0 | Variables d'environnement |

#### CatÃ©gorie 5ï¸âƒ£ : Utilitaires

| Package | Version | Usage |
|---------|---------|-------|
| `tqdm` | >= 4.66 | Barres de progression |
| `tenacity` | >= 8.2 | Retry logic (rÃ©essayer en cas d'Ã©chec) |

### ğŸ³ Docker Compose (Services)

**Services requis** :
- **PostgreSQL 15** : Base de donnÃ©es relationnelle
- **MinIO** : Object Storage S3-compatible (DataLake)
- **Redis** (optionnel) : Cache pour futures optimisations

**Fichier `docker-compose.yml`** :
```yaml
services:
  postgres:
    image: postgres:15
    environment:
      POSTGRES_USER: ds_user
      POSTGRES_PASSWORD: ds_pass
      POSTGRES_DB: datasens
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

  minio:
    image: minio/minio
    command: server /data
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: miniouser
      MINIO_ROOT_PASSWORD: miniosecret
    volumes:
      - minio_data:/data
```

---

## ğŸ““ DÃ©tail notebook par notebook {#notebook-par-notebook}

### Notebook 1 : `01_setup_env.ipynb`

**ğŸ¯ Objectif** : VÃ©rifier que l'environnement est prÃªt pour exÃ©cuter le pipeline ETL

#### Contenu dÃ©taillÃ©

1. **Markdown intro** : Objectifs, RGPD, gouvernance
2. **VÃ©rification Python** :
   - Version Python (`sys.version`)
   - Liste packages installÃ©s (`pip list`)
   - Versions Pandas/SQLAlchemy
3. **CrÃ©ation arborescence** :
   - `data/raw/`, `data/silver/`, `data/gold/`
   - `logs/`, `docs/`, `notebooks/`
4. **Configuration `.env`** :
   - Chargement variables PostgreSQL
   - Chargement variables MinIO
   - CrÃ©ation `.env.example` si `.env` manquant
5. **Test connexion PostgreSQL** :
   ```python
   with engine.connect() as conn:
       result = conn.execute(text("SELECT 1"))
       print("âœ… PostgreSQL OK")
   ```
6. **Test connexion MinIO** :
   ```python
   minio_client = Minio(...)
   ensure_bucket("datasens-raw")
   print("âœ… MinIO OK")
   ```
7. **VÃ©rification Git** :
   - `git status`
   - `git init` si nÃ©cessaire

#### âœ… CritÃ¨res de validation

- Python 3.11+ dÃ©tectÃ©
- Tous les packages requis installÃ©s
- PostgreSQL connectÃ© (test `SELECT 1` OK)
- MinIO connectÃ© (bucket `datasens-raw` crÃ©Ã©)
- Arborescence projet crÃ©Ã©e
- Git initialisÃ©

---

### Notebook 2 : `02_schema_create.ipynb`

**ğŸ¯ Objectif** : CrÃ©er le schÃ©ma PostgreSQL complet (18 tables Merise) avec contraintes, index et donnÃ©es de rÃ©fÃ©rence

#### Contenu dÃ©taillÃ©

1. **Markdown intro** : Objectifs, RGPD, gouvernance
2. **Rappel MCD/MLD** :
   - Liste des 18 tables cibles
   - SchÃ©ma Mermaid ER simplifiÃ©
3. **Connexion PostgreSQL** : MÃªme configuration que notebook 1
4. **DDL complet** : CREATE TABLE pour les 18 tables :
   - **Collecte** : `type_donnee`, `source`, `flux`
   - **Corpus** : `document`, `territoire`
   - **Contexte** : `type_meteo`, `meteo`, `type_indicateur`, `source_indicateur`, `indicateur`
   - **ThÃ¨mes/Ã©vÃ©nements** : `theme`, `evenement`, `document_evenement`
   - **Gouvernance** : `pipeline`, `etape_etl`
   - **Utilisateurs** : `utilisateur`
   - **QualitÃ©** : `qc_rule`, `qc_result`
5. **Index et contraintes** :
   - Index sur `hash_fingerprint` (dÃ©duplication rapide)
   - Index sur dates (requÃªtes temporelles)
   - Index sur clÃ©s Ã©trangÃ¨res
6. **Insertion rÃ©fÃ©rentiels** :
   - `type_donnee` : Fichier plat, Base de donnÃ©es, API, Web Scraping, Big Data
   - `type_meteo` : CLEAR, CLOUDS, RAIN, SNOW, THUNDERSTORM, FOG
   - `type_indicateur` : POPULATION, REVENU_MEDIAN, TAUX_CHOMAGE, SUPERFICIE
   - `source_indicateur` : INSEE, IGN, data.gouv.fr
   - `theme` : Politique, Ã‰conomie, SociÃ©tÃ©, Environnement, SantÃ©, Sport, Culture, Technologie
   - `qc_rule` : RÃ¨gles de qualitÃ© (No duplicates, No NULL titles, Date range valid)
7. **ContrÃ´les** :
   - Liste toutes les tables crÃ©Ã©es (`\dt` Ã©quivalent)
   - Compte entrÃ©es par table

#### âœ… CritÃ¨res de validation

- 18 tables crÃ©Ã©es avec succÃ¨s
- Tous les index crÃ©Ã©s
- RÃ©fÃ©rentiels insÃ©rÃ©s (ON CONFLICT DO NOTHING)
- Contraintes FK/UNIQUE/CHECK vÃ©rifiÃ©es
- Aucune erreur DDL

---

### Notebook 3 : `03_ingest_sources.ipynb`

**ğŸ¯ Objectif** : IngÃ©rer rÃ©ellement les 5 types de sources avec traÃ§abilitÃ© complÃ¨te (logs, manifests, MinIO)

#### Architecture Pipeline (rÃ©fÃ©rence `datasens_E1_v2.ipynb`)

**âœ… Ce notebook suit l'architecture du pipeline existant** :

1. **SystÃ¨me de logging structurÃ©** :
   - `logs/collecte_YYYYMMDD_HHMMSS.log` (toutes opÃ©rations)
   - `logs/errors_YYYYMMDD_HHMMSS.log` (erreurs + traceback)
   - Logger avec 3 handlers (file, error, console)

2. **MinIO DataLake** :
   - Upload automatique fichiers bruts â†’ `s3://datasens-raw/`
   - Helper `minio_upload(local_path, dest_key)`

3. **PostgreSQL** :
   - Insertion structurÃ©e avec traÃ§abilitÃ© (flux, manifests)
   - Helpers : `create_flux()`, `insert_documents()`, `ensure_territoire()`, `get_source_id()`

4. **DÃ©duplication** :
   - Hash SHA-256 pour Ã©viter doublons
   - `ON CONFLICT (hash_fingerprint) DO NOTHING`

5. **RGPD** :
   - Pas de donnÃ©es personnelles directes
   - Hash SHA-256 des auteurs si nÃ©cessaire

#### Contenu dÃ©taillÃ©

1. **Markdown intro** : Objectifs, plan d'ingestion, RGPD, gouvernance

2. **Configuration et setup** :
   - Imports (logging, pandas, sqlalchemy, minio, etc.)
   - Connexions PostgreSQL/MinIO
   - **SystÃ¨me de logging** (alignÃ© sur `datasens_E1_v2.ipynb`)
   - Helpers : `ts()`, `sha256()`, `get_source_id()`, `create_flux()`, `ensure_territoire()`, `insert_documents()`

3. **Source 1 : Fichier plat (Kaggle CSV)** :
   - VÃ©rifie/crÃ©e Ã©chantillon Kaggle CSV
   - Charge donnÃ©es, calcule `hash_fingerprint`
   - **Split 50/50** : 50% â†’ PostgreSQL, 50% â†’ MinIO (brut)
   - Sauvegarde locale + upload MinIO
   - Insertion PostgreSQL via `create_flux()` + `insert_documents()`
   - Logging dÃ©taillÃ©

4. **Source 2 : Base de donnÃ©es (Kaggle SQLite)** *(Ã  implÃ©menter)* :
   - Connexion SQLite Kaggle
   - Export â†’ DataFrame
   - Insertion PostgreSQL

5. **Source 3 : API (OpenWeatherMap)** *(Ã  implÃ©menter)* :
   - Appel API OWM pour communes test
   - CrÃ©ation entrÃ©es `meteo` + `flux`
   - Upload JSON rÃ©ponse brute â†’ MinIO

6. **Source 4 : Web Scraping (MonAvisCitoyen)** *(Ã  implÃ©menter)* :
   - Scraping Ã©thique (respect robots.txt, throttle)
   - Hash auteur SHA-256 (RGPD)
   - CrÃ©ation entrÃ©es `document` + `flux`

7. **Source 5 : Big Data (GDELT GKG)** *(Ã  implÃ©menter)* :
   - TÃ©lÃ©chargement Ã©chantillon journalier
   - Filtrage France
   - CrÃ©ation entrÃ©es `evenement` + `document_evenement`

8. **Manifest JSON** :
   - GÃ©nÃ¨re manifest par run avec :
     - Timestamp
     - Sources ingÃ©rÃ©es
     - Compteurs documents
     - Chemins MinIO
     - Statut (success/error)
   - Sauvegarde locale + upload MinIO

#### âœ… CritÃ¨res de validation

- 5 sources rÃ©ellement ingÃ©rÃ©es (mÃªme en Ã©chantillon)
- Logs structurÃ©s gÃ©nÃ©rÃ©s (`logs/collecte_*.log`)
- Manifests JSON crÃ©Ã©s pour chaque run
- Fichiers bruts uploadÃ©s vers MinIO
- Documents insÃ©rÃ©s dans PostgreSQL avec traÃ§abilitÃ© (`flux` table)
- DÃ©duplication fonctionnelle (pas de doublons)

---

### Notebook 4 : `04_crud_tests.ipynb`

**ğŸ¯ Objectif** : DÃ©montrer les opÃ©rations CRUD complÃ¨tes (Create, Read, Update, Delete) sur plusieurs tables

#### Contenu dÃ©taillÃ©

1. **Markdown intro** : Objectifs, RGPD, gouvernance

2. **Configuration** : Connexion PostgreSQL (mÃªme config que notebooks prÃ©cÃ©dents)

3. **CRUD "C" (Create)** :
   - Insertion d'un document
   - Insertion d'une mÃ©tÃ©o
   - Insertion d'un indicateur
   - Affichage rÃ©sultats (counts avant/aprÃ¨s)

4. **CRUD "R" (Read)** :
   - RequÃªte jointe : documents + territoire + source
   - RequÃªte mÃ©tÃ©o rÃ©cente par territoire
   - RequÃªte Ã©vÃ©nements par thÃ¨me
   - Affichage rÃ©sultats (DataFrame pandas)

5. **CRUD "U" (Update)** :
   - Modification langue d'un document
   - Modification titre d'un document
   - Modification tempÃ©rature d'une mÃ©tÃ©o
   - VÃ©rification changements

6. **CRUD "D" (Delete)** :
   - Suppression contrÃ´lÃ©e d'un document
   - VÃ©rification ON DELETE CASCADE (si applicable)
   - Counts avant/aprÃ¨s

7. **Quality Checks** :
   - DÃ©tection doublons via `hash_fingerprint`
   - Pourcentage NULL par colonne critique (document table)
   - Tableau rÃ©capitulatif

8. **KPIs** :
   - Nombre documents par source
   - Nombre Ã©vÃ©nements par thÃ¨me
   - Volume donnÃ©es par type_donnee

#### âœ… CritÃ¨res de validation

- CREATE : Documents/mÃ©tÃ©o/indicateurs insÃ©rÃ©s avec succÃ¨s
- READ : RequÃªtes jointes fonctionnent correctement
- UPDATE : Modifications appliquÃ©es et vÃ©rifiÃ©es
- DELETE : Suppressions contrÃ´lÃ©es (pas de cascade non dÃ©sirÃ©e)
- Quality checks : Doublons dÃ©tectÃ©s, % NULL calculÃ©
- KPIs : MÃ©triques calculÃ©es et affichÃ©es

---

### Notebook 5 : `05_snapshot_and_readme.ipynb`

**ğŸ¯ Objectif** : Bilan E1, export DDL/CSV, crÃ©ation tag Git, roadmap E2/E3

#### Contenu dÃ©taillÃ©

1. **Markdown intro** : Objectifs, bilan E1

2. **Configuration** : Connexion PostgreSQL

3. **Bilan E1** :
   - Statistiques tables crÃ©Ã©es
   - Total documents ingÃ©rÃ©s
   - Total flux crÃ©Ã©s
   - Total sources utilisÃ©es
   - Liste des 5 types de sources ingÃ©rÃ©s
   - Affichage tableau rÃ©capitulatif

4. **Export DDL** :
   - GÃ©nÃ¨re `docs/e1_schema.sql` (simplifiÃ©)
   - Note : Utiliser `pg_dump` pour export complet

5. **Export CSV** :
   - Export Ã©chantillons tables clÃ©s vers `data/gold/` :
     - `document` (1000 premiÃ¨res lignes)
     - `source`
     - `flux`
     - `territoire`
     - `meteo` (si disponible)

6. **Git Tag** :
   - CrÃ©e tag `E1_REAL_YYYYMMDD` (ex: `E1_REAL_20251029`)
   - Instructions manuelles si Ã©chec automatique

7. **Roadmap E2/E3** :
   - **E2 (Enrichissement IA)** :
     - Analyse sentiment avancÃ©e (spaCy, transformers)
     - Extraction entitÃ©s nommÃ©es (NER)
     - Classification automatique thÃ¨mes
     - CorrÃ©lations Ã©vÃ©nements/sentiments
   - **E3 (Production & Visualisation)** :
     - Automatisation collecte (GitHub Actions, Airflow)
     - Dashboard Grafana temps rÃ©el
     - API REST (FastAPI)
     - Machine Learning (prÃ©diction sentiments)

8. **Validation E1** :
   - Checklist critÃ¨res validation E1
   - Confirmations (âœ…/âŒ)

#### âœ… CritÃ¨res de validation

- Bilan E1 complet avec statistiques
- DDL exportÃ© vers `docs/e1_schema.sql`
- CSV Ã©chantillons exportÃ©s vers `data/gold/`
- Tag Git crÃ©Ã© (ou instructions fournies)
- Roadmap E2/E3 documentÃ©e

---

## ğŸ”§ Architecture Pipeline (RÃ©fÃ©rence datasens_E1_v2.ipynb) {#architecture-pipeline}

### Principes fondamentaux

**Ce guide suit l'architecture dÃ©montrÃ©e dans `datasens_E1_v2.ipynb`** :

1. **Logging structurÃ©** :
   - Fichiers logs persistants (`logs/collecte_*.log`, `logs/errors_*.log`)
   - Traceback complet pour erreurs
   - Timestamps UTC ISO

2. **Dual Storage** :
   - **PostgreSQL** : DonnÃ©es structurÃ©es (mÃ©tadonnÃ©es, documents nettoyÃ©s)
   - **MinIO** : Fichiers bruts (CSV, JSON, archives)

3. **TraÃ§abilitÃ©** :
   - Table `flux` : Enregistre chaque collecte (date, format, manifest_uri)
   - Manifests JSON : MÃ©tadonnÃ©es de chaque run (sources, compteurs, chemins)
   - Hash SHA-256 : DÃ©duplication + RGPD

4. **Robustesse** :
   - Try/except par source (1 source qui fail â‰  pipeline qui crash)
   - Retry logic (tenacity) pour APIs
   - Fallback gracieux (continue avec autres sources)

### Diagramme de flux complet

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ğŸŒ SOURCES EXTERNES                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ“ Fichier plat (Kaggle CSV)                                 â”‚
â”‚  ğŸ—„ï¸ Base donnÃ©es (Kaggle SQLite)                             â”‚
â”‚  ğŸŒ API (OpenWeatherMap, NewsAPI, RSS)                        â”‚
â”‚  ğŸ•¸ï¸ Web Scraping (MonAvisCitoyen, Reddit, etc.)               â”‚
â”‚  ğŸ“Š Big Data (GDELT GKG)                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ğŸ“¥ EXTRACT (03_ingest_sources.ipynb)        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Collecte brute depuis chaque source                         â”‚
â”‚  â€¢ Logger.info("Source X collectÃ©e")                           â”‚
â”‚  â€¢ Gestion erreurs (try/except + log_error)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    âš™ï¸ TRANSFORM                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Calcul hash_fingerprint (SHA-256)                         â”‚
â”‚  2. Nettoyage texte (regex, HTML)                              â”‚
â”‚  3. Normalisation format ({titre, texte, date, ...})          â”‚
â”‚  4. DÃ©duplication (ON CONFLICT hash_fingerprint)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚                  â”‚                         â”‚
                     â–¼                  â–¼                         â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   MinIO DataLake  â”‚  â”‚   PostgreSQL      â”‚  â”‚      Logs        â”‚
        â”‚   (Bruts 50%)     â”‚  â”‚   (StructurÃ©)     â”‚  â”‚    (Audit)       â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚                  â”‚                         â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ğŸ“Š ANALYTICS                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ CRUD complet (04_crud_tests.ipynb)                         â”‚
â”‚  â€¢ Quality checks (doublons, % NULL)                           â”‚
â”‚  â€¢ KPIs (documents/source, Ã©vÃ©nements/thÃ¨me)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Fonctions helpers (rÃ©fÃ©rence `datasens_E1_v2.ipynb`)

**Toutes ces fonctions sont intÃ©grÃ©es dans `03_ingest_sources.ipynb`** :

```python
def ts() -> str:
    """Timestamp UTC ISO compact"""
    return datetime.now(timezone.utc).strftime("%Y%m%dT%H%M%SZ")

def sha256(s: str) -> str:
    """Hash SHA-256 pour dÃ©duplication"""
    return hashlib.sha256(s.encode("utf-8")).hexdigest()

def get_source_id(conn, nom: str) -> int:
    """RÃ©cupÃ¨re id_source depuis nom"""
    result = conn.execute(text("SELECT id_source FROM source WHERE nom = :nom"), {"nom": nom}).fetchone()
    return result[0] if result else None

def create_flux(conn, id_source: int, format_type: str = "csv", manifest_uri: str = None) -> int:
    """CrÃ©e un flux et retourne id_flux"""
    result = conn.execute(text("""
        INSERT INTO flux (id_source, format, manifest_uri)
        VALUES (:id_source, :format, :manifest_uri)
        RETURNING id_flux
    """), {"id_source": id_source, "format": format_type, "manifest_uri": manifest_uri})
    return result.scalar()

def ensure_territoire(conn, ville: str, code_insee: str = None, lat: float = None, lon: float = None) -> int:
    """CrÃ©e ou rÃ©cupÃ¨re un territoire"""
    result = conn.execute(text("SELECT id_territoire FROM territoire WHERE ville = :ville"), {"ville": ville}).fetchone()
    if result:
        return result[0]
    result = conn.execute(text("""
        INSERT INTO territoire (ville, code_insee, lat, lon)
        VALUES (:ville, :code_insee, :lat, :lon)
        RETURNING id_territoire
    """), {"ville": ville, "code_insee": code_insee, "lat": lat, "lon": lon})
    return result.scalar()

def insert_documents(conn, docs: list) -> int:
    """Insertion batch avec gestion doublons"""
    inserted = 0
    for doc in docs:
        result = conn.execute(text("""
            INSERT INTO document (id_flux, id_territoire, titre, texte, langue, date_publication, hash_fingerprint)
            VALUES (:id_flux, :id_territoire, :titre, :texte, :langue, :date_publication, :hash_fingerprint)
            ON CONFLICT (hash_fingerprint) DO NOTHING
            RETURNING id_doc
        """), doc)
        if result.scalar():
            inserted += 1
    return inserted

def minio_upload(local_path: Path, dest_key: str) -> str:
    """Upload fichier vers MinIO DataLake"""
    ensure_bucket(MINIO_BUCKET)
    minio_client.fput_object(MINIO_BUCKET, dest_key, str(local_path))
    return f"s3://{MINIO_BUCKET}/{dest_key}"
```

---

## ğŸ“Š Diagrammes et visualisations {#diagrammes}

### Diagramme MCD/MLD simplifiÃ© (18 tables E1)

```mermaid
erDiagram
    TYPE_DONNEE ||--o{ SOURCE : "a pour"
    SOURCE ||--o{ FLUX : "gÃ©nÃ¨re"
    FLUX ||--o{ DOCUMENT : "contient"
    TERRITOIRE ||--o{ DOCUMENT : "gÃ©olocalise"
    TERRITOIRE ||--o{ METEO : "mesure"
    TERRITOIRE ||--o{ INDICATEUR : "agrÃ¨ge"
    THEME ||--o{ EVENEMENT : "classe"
    DOCUMENT ||--o{ DOCUMENT_EVENEMENT : "refÃ¨re"
    EVENEMENT ||--o{ DOCUMENT_EVENEMENT : "associe"
    TYPE_METEO ||--o{ METEO : "catÃ©gorise"
    TYPE_INDICATEUR ||--o{ INDICATEUR : "dÃ©finit"
    SOURCE_INDICATEUR ||--o{ INDICATEUR : "provoque"
    PIPELINE ||--o{ ETAPE_ETL : "compose"
    QC_RULE ||--o{ QC_RESULT : "valide"
    FLUX ||--o{ QC_RESULT : "teste"
```

### Architecture stockage dual

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  ğŸ›ï¸ ARCHITECTURE DE STOCKAGE                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“Š PostgreSQL (DonnÃ©es StructurÃ©es)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Tables relationnelles (18 tables Merise)                     â”‚
â”‚  â€¢ Metadata (source, flux, territoire)                          â”‚
â”‚  â€¢ Documents nettoyÃ©s (hash_fingerprint unique)                â”‚
â”‚  â€¢ Contexte (mÃ©tÃ©o, indicateurs)                               â”‚
â”‚  â€¢ Ã‰vÃ©nements (evenement, document_evenement)                  â”‚
â”‚  â€¢ Gouvernance (pipeline, etape_etl)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â–²
                        â”‚
                   INSERT
                        â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   03_ingest_sources.ipynb                              â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                   UPLOAD
                        â”‚
                        â–¼
â˜ï¸ MinIO (Fichiers Bruts - DataLake)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  s3://datasens-raw/                                            â”‚
â”‚  â”œâ”€â”€ kaggle/                                                   â”‚
â”‚  â”‚   â””â”€â”€ kaggle_bilingual_YYYYMMDDTHHMMSSZ.csv                â”‚
â”‚  â”œâ”€â”€ api/                                                      â”‚
â”‚  â”‚   â”œâ”€â”€ owm/                                                  â”‚
â”‚  â”‚   â”‚   â””â”€â”€ owm_YYYYMMDDTHHMMSSZ.csv                         â”‚
â”‚  â”‚   â””â”€â”€ newsapi/                                              â”‚
â”‚  â”œâ”€â”€ scraping/                                                 â”‚
â”‚  â”‚   â””â”€â”€ multi/scraping_multi_YYYYMMDDTHHMMSSZ.csv            â”‚
â”‚  â”œâ”€â”€ gdelt/                                                    â”‚
â”‚  â”‚   â””â”€â”€ YYYYMMDDHHMMSS.gkg.csv.zip                          â”‚
â”‚  â””â”€â”€ manifests/                                                â”‚
â”‚      â””â”€â”€ manifest_YYYYMMDDTHHMMSSZ.json                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ Troubleshooting {#troubleshooting}

### ProblÃ¨me 1 : `ModuleNotFoundError`

**SymptÃ´me** :
```
ModuleNotFoundError: No module named 'pandas'
```

**Solution** :
```bash
pip install -r requirements.txt
```

---

### ProblÃ¨me 2 : `OperationalError: could not connect to server`

**SymptÃ´me** :
```
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not connect to server
```

**Causes possibles** :
1. PostgreSQL n'est pas dÃ©marrÃ©
2. Mauvais host/port dans `.env`
3. Firewall bloque le port 5432

**Solutions** :

1. **VÃ©rifier Docker Compose** :
   ```bash
   docker-compose ps
   # Si postgres n'est pas UP, lancer :
   docker-compose up -d postgres
   ```

2. **VÃ©rifier credentials `.env`** :
   ```bash
   # Windows PowerShell
   Get-Content .env | Select-String "POSTGRES"
   ```

3. **Test connexion manuelle** :
   ```bash
   # Depuis PowerShell
   psql -h localhost -U ds_user -d datasens
   ```

---

### ProblÃ¨me 3 : MinIO non accessible

**SymptÃ´me** :
```
âš ï¸ MinIO : Erreur connexion
```

**Solutions** :

1. **VÃ©rifier Docker Compose** :
   ```bash
   docker-compose ps
   # Si minio n'est pas UP, lancer :
   docker-compose up -d minio
   ```

2. **VÃ©rifier endpoint `.env`** :
   ```bash
   # Doit Ãªtre : MINIO_ENDPOINT=http://localhost:9000
   ```

3. **AccÃ¨s console MinIO** :
   - Ouvrir `http://localhost:9001` dans navigateur
   - Login : `miniouser` / `miniosecret`

---

### ProblÃ¨me 4 : Erreur DDL "table already exists"

**SymptÃ´me** :
```
psycopg2.errors.DuplicateTable: relation "document" already exists
```

**Solutions** :

1. **Supprimer tables existantes** (âš ï¸ perte de donnÃ©es) :
   ```sql
   DROP TABLE IF EXISTS document CASCADE;
   -- RÃ©pÃ©ter pour toutes les tables
   ```

2. **OU utiliser CREATE TABLE IF NOT EXISTS** (dÃ©jÃ  dans le code) :
   ```sql
   CREATE TABLE IF NOT EXISTS document (...);
   ```

---

### ProblÃ¨me 5 : Doublons dÃ©tectÃ©s lors de l'ingestion

**SymptÃ´me** :
```
0 documents insÃ©rÃ©s (tous des doublons)
```

**Explication** :
- Normal si vous rÃ©exÃ©cutez `03_ingest_sources.ipynb` plusieurs fois
- La contrainte `UNIQUE (hash_fingerprint)` + `ON CONFLICT DO NOTHING` Ã©vite les doublons

**VÃ©rification** :
```python
# Dans 04_crud_tests.ipynb, cellule Quality Checks
# VÃ©rifier qu'il n'y a pas de vrais doublons :
SELECT hash_fingerprint, COUNT(*)
FROM document
GROUP BY hash_fingerprint
HAVING COUNT(*) > 1;
# â†’ Doit retourner 0 lignes
```

---

## ğŸ“– Glossaire et concepts {#glossaire}

### Termes techniques expliquÃ©s

| Terme | DÃ©finition simple | Exemple |
|-------|-------------------|---------|
| **ETL** | Extract-Transform-Load : Aspirer, nettoyer, ranger | RSS â†’ nettoyage â†’ PostgreSQL |
| **MCD/MLD** | ModÃ¨le Conceptuel/Logique de DonnÃ©es (Merise) | Diagramme relations entre tables |
| **DataLake** | Stockage fichiers bruts (S3-like) | MinIO : `s3://datasens-raw/kaggle/` |
| **Hash SHA-256** | Empreinte digitale unique (64 caractÃ¨res) | `a3f5c9d2e4b6f8a1c3e5d7b9f2a4c6e8` |
| **CRUD** | Create, Read, Update, Delete (opÃ©rations de base) | INSERT, SELECT, UPDATE, DELETE |
| **Manifest** | Fichier JSON mÃ©tadonnÃ©es d'une collecte | `manifest_20251029T123337Z.json` |
| **Flux** | Enregistrement d'une collecte (table `flux`) | Source + date + format + manifest_uri |
| **Fingerprint** | Hash SHA-256 pour dÃ©duplication | `hash_fingerprint VARCHAR(64) UNIQUE` |

### Concepts Python avancÃ©s

#### Context Manager (`with`)

```python
with engine.connect() as conn:
    result = conn.execute(text("SELECT 1"))
    # Connexion fermÃ©e automatiquement ici (mÃªme en cas d'erreur)
```

**Avantage** : Gestion automatique ressources (Ã©vite fuites de connexions)

---

#### ParamÃ¨tres nommÃ©s SQL (SQLAlchemy)

```python
conn.execute(text("SELECT * FROM document WHERE id = :id"), {"id": 123})
```

**Avantage** : SÃ©curitÃ© (Ã©vite injection SQL) + LisibilitÃ©

---

#### ON CONFLICT DO NOTHING

```sql
INSERT INTO document (hash_fingerprint, ...)
VALUES (:hash, ...)
ON CONFLICT (hash_fingerprint) DO NOTHING;
```

**Avantage** : DÃ©duplication automatique (pas d'erreur si doublon)

---

## âœ… Checklist validation finale

Avant de prÃ©senter au jury, vÃ©rifier :

### Environnement
- [ ] Python 3.11+ installÃ©
- [ ] Tous les packages (`pip install -r requirements.txt`)
- [ ] Docker Compose lancÃ© (`docker-compose up -d`)
- [ ] `.env` configurÃ© (PostgreSQL, MinIO, API keys)

### Notebooks exÃ©cutÃ©s
- [ ] `01_setup_env.ipynb` : Connexions OK
- [ ] `02_schema_create.ipynb` : 18 tables crÃ©Ã©es
- [ ] `03_ingest_sources.ipynb` : 5 sources ingÃ©rÃ©es
- [ ] `04_crud_tests.ipynb` : CRUD dÃ©montrÃ©
- [ ] `05_snapshot_and_readme.ipynb` : Exports OK

### DonnÃ©es
- [ ] Documents en base (vÃ©rifier `SELECT COUNT(*) FROM document`)
- [ ] Flux crÃ©Ã©s (vÃ©rifier `SELECT COUNT(*) FROM flux`)
- [ ] Manifests JSON gÃ©nÃ©rÃ©s (`data/raw/manifests/`)
- [ ] Logs gÃ©nÃ©rÃ©s (`logs/collecte_*.log`)

### Git
- [ ] Tag E1 crÃ©Ã© (`git tag E1_REAL_YYYYMMDD`)
- [ ] Commit final effectuÃ©

---

## ğŸ¤ PrÃ©sentation jury - Script 2 minutes

> "DataSens E1, c'est un **socle de donnÃ©es pour entraÃ®nement IA** avec 5 notebooks Jupyter structurÃ©s.
>
> **Notebook 1** : Setup environnement, connexions PostgreSQL + MinIO
>
> **Notebook 2** : CrÃ©ation schÃ©ma Merise (18 tables relationnelles) avec contraintes et index
>
> **Notebook 3** : Ingestion rÃ©elle de 5 types de sources :
> - Fichier plat (Kaggle CSV â†’ 50% PostgreSQL, 50% MinIO)
> - Base de donnÃ©es (Kaggle SQLite â†’ PostgreSQL)
> - API (OpenWeatherMap â†’ mÃ©tÃ©o + flux)
> - Web Scraping (MonAvisCitoyen â†’ documents)
> - Big Data (GDELT GKG â†’ Ã©vÃ©nements)
>
> **Notebook 4** : DÃ©monstration CRUD complet + contrÃ´les qualitÃ©
>
> **Notebook 5** : Bilan E1, exports DDL/CSV, roadmap E2/E3
>
> **RÃ©sultat** : Pipeline ETL reproductible, traÃ§abilitÃ© complÃ¨te (logs, manifests, Git), architecture scalable pour industrialisation dataset Ã  grande Ã©chelle."

---

## ğŸ“š Ressources complÃ©mentaires

### Documentation officielle
- **SQLAlchemy** : https://docs.sqlalchemy.org/
- **Pandas** : https://pandas.pydata.org/docs/
- **MinIO** : https://min.io/docs/
- **PostgreSQL** : https://www.postgresql.org/docs/

### Projets de rÃ©fÃ©rence
- `notebooks/datasens_E1_v2.ipynb` : Architecture pipeline complÃ¨te
- `docs/ARCHITECTURE_ETL.md` : Architecture ETL dÃ©taillÃ©e
- `docs/ARCHITECTURE_PIPELINE_E1.md` : Alignement pipeline

---

**ğŸ“ Bonne chance pour la prÃ©sentation au jury !** ğŸš€
