```text
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                                                                              ‚ïë
‚ïë                                                                              ‚ïë
‚ïë     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó     ‚ïë
‚ïë     ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ïö‚ïê‚ïê‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù     ‚ïë
‚ïë     ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ïî‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó     ‚ïë
‚ïë     ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïë‚ïö‚ïê‚ïê‚ïê‚ïê‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù  ‚ñà‚ñà‚ïë‚ïö‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë‚ïö‚ïê‚ïê‚ïê‚ïê‚ñà‚ñà‚ïë     ‚ïë
‚ïë     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë ‚ïö‚ñà‚ñà‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë     ‚ïë
‚ïë     ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù  ‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù   ‚ïö‚ïê‚ïù   ‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù     ‚ïë
‚ïë                                                                              ‚ïë
‚ïë                                                                              ‚ïë
‚ïë                   ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó                     ‚ïë
‚ïë                   ‚ïë   GUIDE TECHNIQUE COMPLET - E1     ‚ïë                     ‚ïë
‚ïë                   ‚ïë        Structure 5 Notebooks       ‚ïë                     ‚ïë
‚ïë                   ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù                     ‚ïë
‚ïë                                                                              ‚ïë
‚ïë                      üìä Pipeline ETL Multi-Sources                           ‚ïë
‚ïë                      üóÑÔ∏è PostgreSQL Merise (18 tables)                        ‚ïë
‚ïë                      üíæ MinIO DataLake + Logging                             ‚ïë
‚ïë                      üìà CRUD + Visualisations + Qualit√©                      ‚ïë
‚ïë                                                                              ‚ïë
‚ïë                      ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                       ‚ïë
‚ïë                           Projet Certifiant 2025                             ‚ïë
‚ïë                      ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                       ‚ïë
‚ïë                                                                              ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
```

# üöÄ Guide Technique DataSens E1 - Structure 5 Notebooks

> **Approche p√©dagogique** : 5 notebooks Jupyter structur√©s, code inline simple et transparent. Architecture de pipeline align√©e sur `datasens_E1_v2.ipynb`. Tout visible pour le jury ! üí™

---

## üì¶ Table des Mati√®res

1. [Vue d'ensemble du projet](#vue-densemble)
2. [Architecture du projet - 5 Notebooks](#architecture-5-notebooks)
3. [Stack technique et d√©pendances](#stack-technique)
4. [D√©taill√© notebook par notebook](#notebook-par-notebook)
5. [Architecture Pipeline (R√©f√©rence datasens_E1_v2.ipynb)](#architecture-pipeline)
6. [Diagrammes et visualisations](#diagrammes)
7. [Troubleshooting](#troubleshooting)
8. [Glossaire et concepts](#glossaire)

---

## üéØ Vue d'ensemble du projet {#vue-densemble}

### DataSens E1 : Socle de donn√©es pour entra√Ænement IA

**Objectif principal** : Cr√©er un jeu de donn√©es pr√©-annot√© pour optimiser l'entra√Ænement de mod√®les IA, minimiser la d√©rive et les hallucinations.

**üéì Approche acad√©mique** :
- ‚úÖ **5 notebooks Jupyter** structur√©s cellule par cellule
- ‚úÖ Code **simple et lisible** dans les cellules (pas de modules `.py` cach√©s)
- ‚úÖ **Merise mod√©lisation** : MCD ‚Üí MLD ‚Üí MPD cibl√©
- ‚úÖ **5 types de sources** r√©ellement ing√©r√©s (fichier plat, BDD, API, scraping, big data)
- ‚úÖ **CRUD complet** d√©montr√© dans un notebook d√©di√©
- ‚úÖ **Tra√ßabilit√©** : flux, manifests JSON, logs structur√©s, Git versioning

**Le but** : D√©montrer au jury qu'on ma√Ætrise la collecte multi-sources, le mod√®le relationnel Merise, et la gouvernance des donn√©es avec du code propre et compr√©hensible.

---

## üèóÔ∏è Architecture du projet - 5 Notebooks {#architecture-5-notebooks}

### Structure modulaire progressive

```
üìÇ notebooks/
‚îú‚îÄ‚îÄ üìì 01_setup_env.ipynb           ‚Üí Environnement & connexions
‚îú‚îÄ‚îÄ üìì 02_schema_create.ipynb        ‚Üí DDL PostgreSQL (18 tables Merise)
‚îú‚îÄ‚îÄ üìì 03_ingest_sources.ipynb       ‚Üí Ingestion 5 sources + manifests
‚îú‚îÄ‚îÄ üìì 04_crud_tests.ipynb           ‚Üí D√©monstration CRUD complet
‚îî‚îÄ‚îÄ üìì 05_snapshot_and_readme.ipynb ‚Üí Bilan E1 + exports + roadmap E2/E3
```

### Flux d'ex√©cution

```
01_setup_env.ipynb
    ‚Üì (V√©rifie environnement, connexions PG/MinIO)
02_schema_create.ipynb
    ‚Üì (Cr√©e 18 tables, r√©f√©rentiels, index)
03_ingest_sources.ipynb
    ‚Üì (Ing√®re 5 sources, cr√©e manifests, logs)
04_crud_tests.ipynb
    ‚Üì (D√©montre CREATE/READ/UPDATE/DELETE)
05_snapshot_and_readme.ipynb
    ‚Üì (Exporte DDL, CSV, cr√©e tag Git)
```

### üìä Tableau r√©capitulatif des notebooks

| Notebook | Objectif | Dur√©e estim√©e | D√©pendances |
|----------|----------|---------------|-------------|
| **01_setup_env** | V√©rifier environnement Python, connexions PostgreSQL/MinIO, Git init | 5 min | Python 3.11+, Docker |
| **02_schema_create** | Cr√©er sch√©ma PostgreSQL (18 tables), index, r√©f√©rentiels | 10 min | 01_setup_env ex√©cut√© |
| **03_ingest_sources** | Ing√©rer 5 sources (Kaggle, OWM, GDELT, etc.) | 30-60 min | 02_schema_create, API keys |
| **04_crud_tests** | D√©montrer CRUD complet + qualit√© | 15 min | 03_ingest_sources ex√©cut√© |
| **05_snapshot_and_readme** | Exporter DDL/CSV, cr√©er tag Git, roadmap | 5 min | Tous les notebooks pr√©c√©dents |

---

## üìã Stack technique et d√©pendances {#stack-technique}

### üêç Python 3.11+ (obligatoire)

**Pourquoi Python 3.11+ ?**
- Performances am√©lior√©es vs 3.9/3.10
- Compatibilit√© avec toutes les libs modernes
- Support natif des annotations de types

### üì¶ D√©pendances principales

#### Cat√©gorie 1Ô∏è‚É£ : Base de donn√©es

| Package | Version | Usage |
|---------|---------|-------|
| `sqlalchemy` | >= 2.0 | ORM Python ‚Üí PostgreSQL |
| `psycopg2-binary` | >= 2.9 | Driver PostgreSQL |
| `python-dotenv` | >= 1.0 | Variables d'environnement `.env` |

**Exemple concret** :
```python
from sqlalchemy import create_engine, text
from dotenv import load_dotenv
load_dotenv()
engine = create_engine(f"postgresql+psycopg2://{user}:{pass}@{host}/{db}")
```

#### Cat√©gorie 2Ô∏è‚É£ : Data processing

| Package | Version | Usage |
|---------|---------|-------|
| `pandas` | >= 2.0 | DataFrames (manipulation donn√©es) |
| `numpy` | >= 1.24 | Calculs num√©riques |

#### Cat√©gorie 3Ô∏è‚É£ : Collecte donn√©es

| Package | Version | Usage |
|---------|---------|-------|
| `requests` | >= 2.31 | HTTP client (APIs REST) |
| `feedparser` | >= 6.0 | Parse RSS/Atom feeds |
| `beautifulsoup4` | >= 4.12 | HTML parsing (web scraping) |
| `selectolax` | >= 0.3 | Alternative rapide √† BeautifulSoup |
| `kaggle` | >= 1.5 | API Kaggle datasets |

#### Cat√©gorie 4Ô∏è‚É£ : Storage & Logging

| Package | Version | Usage |
|---------|---------|-------|
| `minio` | >= 7.0 | Client S3 (DataLake) |
| `python-dotenv` | >= 1.0 | Variables d'environnement |

#### Cat√©gorie 5Ô∏è‚É£ : Utilitaires

| Package | Version | Usage |
|---------|---------|-------|
| `tqdm` | >= 4.66 | Barres de progression |
| `tenacity` | >= 8.2 | Retry logic (r√©essayer en cas d'√©chec) |

### üê≥ Docker Compose (Services)

**Services requis** :
- **PostgreSQL 15** : Base de donn√©es relationnelle
- **MinIO** : Object Storage S3-compatible (DataLake)
- **Redis** (optionnel) : Cache pour futures optimisations

**Fichier `docker-compose.yml`** :
```yaml
services:
  postgres:
    image: postgres:15
    environment:
      POSTGRES_USER: ds_user
      POSTGRES_PASSWORD: ds_pass
      POSTGRES_DB: datasens
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

  minio:
    image: minio/minio
    command: server /data
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: miniouser
      MINIO_ROOT_PASSWORD: miniosecret
    volumes:
      - minio_data:/data
```

---

## üìì D√©tail notebook par notebook {#notebook-par-notebook}

### Notebook 1 : `01_setup_env.ipynb`

**üéØ Objectif** : V√©rifier que l'environnement est pr√™t pour ex√©cuter le pipeline ETL

#### Contenu d√©taill√©

1. **Markdown intro** : Objectifs, RGPD, gouvernance
2. **V√©rification Python** :
   - Version Python (`sys.version`)
   - Liste packages install√©s (`pip list`)
   - Versions Pandas/SQLAlchemy
3. **Cr√©ation arborescence** :
   - `data/raw/`, `data/silver/`, `data/gold/`
   - `logs/`, `docs/`, `notebooks/`
4. **Configuration `.env`** :
   - Chargement variables PostgreSQL
   - Chargement variables MinIO
   - Cr√©ation `.env.example` si `.env` manquant
5. **Test connexion PostgreSQL** :
   ```python
   with engine.connect() as conn:
       result = conn.execute(text("SELECT 1"))
       print("‚úÖ PostgreSQL OK")
   ```
6. **Test connexion MinIO** :
   ```python
   minio_client = Minio(...)
   ensure_bucket("datasens-raw")
   print("‚úÖ MinIO OK")
   ```
7. **V√©rification Git** :
   - `git status`
   - `git init` si n√©cessaire

#### ‚úÖ Crit√®res de validation

- Python 3.11+ d√©tect√©
- Tous les packages requis install√©s
- PostgreSQL connect√© (test `SELECT 1` OK)
- MinIO connect√© (bucket `datasens-raw` cr√©√©)
- Arborescence projet cr√©√©e
- Git initialis√©

---

### Notebook 2 : `02_schema_create.ipynb`

**üéØ Objectif** : Cr√©er le sch√©ma PostgreSQL complet (18 tables Merise) avec contraintes, index et donn√©es de r√©f√©rence

#### Contenu d√©taill√©

1. **Markdown intro** : Objectifs, RGPD, gouvernance
2. **Rappel MCD/MLD** :
   - Liste des 18 tables cibles
   - Sch√©ma Mermaid ER simplifi√©
3. **Connexion PostgreSQL** : M√™me configuration que notebook 1
4. **DDL complet** : CREATE TABLE pour les 18 tables :
   - **Collecte** : `type_donnee`, `source`, `flux`
   - **Corpus** : `document`, `territoire`
   - **Contexte** : `type_meteo`, `meteo`, `type_indicateur`, `source_indicateur`, `indicateur`
   - **Th√®mes/√©v√©nements** : `theme`, `evenement`, `document_evenement`
   - **Gouvernance** : `pipeline`, `etape_etl`
   - **Utilisateurs** : `utilisateur`
   - **Qualit√©** : `qc_rule`, `qc_result`
5. **Index et contraintes** :
   - Index sur `hash_fingerprint` (d√©duplication rapide)
   - Index sur dates (requ√™tes temporelles)
   - Index sur cl√©s √©trang√®res
6. **Insertion r√©f√©rentiels** :
   - `type_donnee` : Fichier plat, Base de donn√©es, API, Web Scraping, Big Data
   - `type_meteo` : CLEAR, CLOUDS, RAIN, SNOW, THUNDERSTORM, FOG
   - `type_indicateur` : POPULATION, REVENU_MEDIAN, TAUX_CHOMAGE, SUPERFICIE
   - `source_indicateur` : INSEE, IGN, data.gouv.fr
   - `theme` : Politique, √âconomie, Soci√©t√©, Environnement, Sant√©, Sport, Culture, Technologie
   - `qc_rule` : R√®gles de qualit√© (No duplicates, No NULL titles, Date range valid)
7. **Contr√¥les** :
   - Liste toutes les tables cr√©√©es (`\dt` √©quivalent)
   - Compte entr√©es par table

#### ‚úÖ Crit√®res de validation

- 18 tables cr√©√©es avec succ√®s
- Tous les index cr√©√©s
- R√©f√©rentiels ins√©r√©s (ON CONFLICT DO NOTHING)
- Contraintes FK/UNIQUE/CHECK v√©rifi√©es
- Aucune erreur DDL

---

### Notebook 3 : `03_ingest_sources.ipynb`

**üéØ Objectif** : Ing√©rer r√©ellement les 5 types de sources avec tra√ßabilit√© compl√®te (logs, manifests, MinIO)

#### Architecture Pipeline (r√©f√©rence `datasens_E1_v2.ipynb`)

**‚úÖ Ce notebook suit l'architecture du pipeline existant** :

1. **Syst√®me de logging structur√©** :
   - `logs/collecte_YYYYMMDD_HHMMSS.log` (toutes op√©rations)
   - `logs/errors_YYYYMMDD_HHMMSS.log` (erreurs + traceback)
   - Logger avec 3 handlers (file, error, console)

2. **MinIO DataLake** :
   - Upload automatique fichiers bruts ‚Üí `s3://datasens-raw/`
   - Helper `minio_upload(local_path, dest_key)`

3. **PostgreSQL** :
   - Insertion structur√©e avec tra√ßabilit√© (flux, manifests)
   - Helpers : `create_flux()`, `insert_documents()`, `ensure_territoire()`, `get_source_id()`

4. **D√©duplication** :
   - Hash SHA-256 pour √©viter doublons
   - `ON CONFLICT (hash_fingerprint) DO NOTHING`

5. **RGPD** :
   - Pas de donn√©es personnelles directes
   - Hash SHA-256 des auteurs si n√©cessaire

#### Contenu d√©taill√©

1. **Markdown intro** : Objectifs, plan d'ingestion, RGPD, gouvernance

2. **Configuration et setup** :
   - Imports (logging, pandas, sqlalchemy, minio, etc.)
   - Connexions PostgreSQL/MinIO
   - **Syst√®me de logging** (align√© sur `datasens_E1_v2.ipynb`)
   - Helpers : `ts()`, `sha256()`, `get_source_id()`, `create_flux()`, `ensure_territoire()`, `insert_documents()`

3. **Source 1 : Fichier plat (Kaggle CSV)** :
   - V√©rifie/cr√©e √©chantillon Kaggle CSV
   - Charge donn√©es, calcule `hash_fingerprint`
   - **Split 50/50** : 50% ‚Üí PostgreSQL, 50% ‚Üí MinIO (brut)
   - Sauvegarde locale + upload MinIO
   - Insertion PostgreSQL via `create_flux()` + `insert_documents()`
   - Logging d√©taill√©

4. **Source 2 : Base de donn√©es (Kaggle SQLite)** *(√† impl√©menter)* :
   - Connexion SQLite Kaggle
   - Export ‚Üí DataFrame
   - Insertion PostgreSQL

5. **Source 3 : API (OpenWeatherMap)** *(√† impl√©menter)* :
   - Appel API OWM pour communes test
   - Cr√©ation entr√©es `meteo` + `flux`
   - Upload JSON r√©ponse brute ‚Üí MinIO

6. **Source 4 : Web Scraping (MonAvisCitoyen)** *(√† impl√©menter)* :
   - Scraping √©thique (respect robots.txt, throttle)
   - Hash auteur SHA-256 (RGPD)
   - Cr√©ation entr√©es `document` + `flux`

7. **Source 5 : Big Data (GDELT GKG)** *(√† impl√©menter)* :
   - T√©l√©chargement √©chantillon journalier
   - Filtrage France
   - Cr√©ation entr√©es `evenement` + `document_evenement`

8. **Manifest JSON** :
   - G√©n√®re manifest par run avec :
     - Timestamp
     - Sources ing√©r√©es
     - Compteurs documents
     - Chemins MinIO
     - Statut (success/error)
   - Sauvegarde locale + upload MinIO

#### ‚úÖ Crit√®res de validation

- 5 sources r√©ellement ing√©r√©es (m√™me en √©chantillon)
- Logs structur√©s g√©n√©r√©s (`logs/collecte_*.log`)
- Manifests JSON cr√©√©s pour chaque run
- Fichiers bruts upload√©s vers MinIO
- Documents ins√©r√©s dans PostgreSQL avec tra√ßabilit√© (`flux` table)
- D√©duplication fonctionnelle (pas de doublons)

---

### Notebook 4 : `04_crud_tests.ipynb`

**üéØ Objectif** : D√©montrer les op√©rations CRUD compl√®tes (Create, Read, Update, Delete) sur plusieurs tables

#### Contenu d√©taill√©

1. **Markdown intro** : Objectifs, RGPD, gouvernance

2. **Configuration** : Connexion PostgreSQL (m√™me config que notebooks pr√©c√©dents)

3. **CRUD "C" (Create)** :
   - Insertion d'un document
   - Insertion d'une m√©t√©o
   - Insertion d'un indicateur
   - Affichage r√©sultats (counts avant/apr√®s)

4. **CRUD "R" (Read)** :
   - Requ√™te jointe : documents + territoire + source
   - Requ√™te m√©t√©o r√©cente par territoire
   - Requ√™te √©v√©nements par th√®me
   - Affichage r√©sultats (DataFrame pandas)

5. **CRUD "U" (Update)** :
   - Modification langue d'un document
   - Modification titre d'un document
   - Modification temp√©rature d'une m√©t√©o
   - V√©rification changements

6. **CRUD "D" (Delete)** :
   - Suppression contr√¥l√©e d'un document
   - V√©rification ON DELETE CASCADE (si applicable)
   - Counts avant/apr√®s

7. **Quality Checks** :
   - D√©tection doublons via `hash_fingerprint`
   - Pourcentage NULL par colonne critique (document table)
   - Tableau r√©capitulatif

8. **KPIs** :
   - Nombre documents par source
   - Nombre √©v√©nements par th√®me
   - Volume donn√©es par type_donnee

#### ‚úÖ Crit√®res de validation

- CREATE : Documents/m√©t√©o/indicateurs ins√©r√©s avec succ√®s
- READ : Requ√™tes jointes fonctionnent correctement
- UPDATE : Modifications appliqu√©es et v√©rifi√©es
- DELETE : Suppressions contr√¥l√©es (pas de cascade non d√©sir√©e)
- Quality checks : Doublons d√©tect√©s, % NULL calcul√©
- KPIs : M√©triques calcul√©es et affich√©es

---

### Notebook 5 : `05_snapshot_and_readme.ipynb`

**üéØ Objectif** : Bilan E1, export DDL/CSV, cr√©ation tag Git, roadmap E2/E3

#### Contenu d√©taill√©

1. **Markdown intro** : Objectifs, bilan E1

2. **Configuration** : Connexion PostgreSQL

3. **Bilan E1** :
   - Statistiques tables cr√©√©es
   - Total documents ing√©r√©s
   - Total flux cr√©√©s
   - Total sources utilis√©es
   - Liste des 5 types de sources ing√©r√©s
   - Affichage tableau r√©capitulatif

4. **Export DDL** :
   - G√©n√®re `docs/e1_schema.sql` (simplifi√©)
   - Note : Utiliser `pg_dump` pour export complet

5. **Export CSV** :
   - Export √©chantillons tables cl√©s vers `data/gold/` :
     - `document` (1000 premi√®res lignes)
     - `source`
     - `flux`
     - `territoire`
     - `meteo` (si disponible)

6. **Git Tag** :
   - Cr√©e tag `E1_REAL_YYYYMMDD` (ex: `E1_REAL_20251029`)
   - Instructions manuelles si √©chec automatique

7. **Roadmap E2/E3** :
   - **E2 (Enrichissement IA)** :
     - Analyse sentiment avanc√©e (spaCy, transformers)
     - Extraction entit√©s nomm√©es (NER)
     - Classification automatique th√®mes
     - Corr√©lations √©v√©nements/sentiments
   - **E3 (Production & Visualisation)** :
     - Automatisation collecte (GitHub Actions, Airflow)
     - Dashboard Grafana temps r√©el
     - API REST (FastAPI)
     - Machine Learning (pr√©diction sentiments)

8. **Validation E1** :
   - Checklist crit√®res validation E1
   - Confirmations (‚úÖ/‚ùå)

#### ‚úÖ Crit√®res de validation

- Bilan E1 complet avec statistiques
- DDL export√© vers `docs/e1_schema.sql`
- CSV √©chantillons export√©s vers `data/gold/`
- Tag Git cr√©√© (ou instructions fournies)
- Roadmap E2/E3 document√©e

---

## üîß Architecture Pipeline (R√©f√©rence datasens_E1_v2.ipynb) {#architecture-pipeline}

### Principes fondamentaux

**Ce guide suit l'architecture d√©montr√©e dans `datasens_E1_v2.ipynb`** :

1. **Logging structur√©** :
   - Fichiers logs persistants (`logs/collecte_*.log`, `logs/errors_*.log`)
   - Traceback complet pour erreurs
   - Timestamps UTC ISO

2. **Dual Storage** :
   - **PostgreSQL** : Donn√©es structur√©es (m√©tadonn√©es, documents nettoy√©s)
   - **MinIO** : Fichiers bruts (CSV, JSON, archives)

3. **Tra√ßabilit√©** :
   - Table `flux` : Enregistre chaque collecte (date, format, manifest_uri)
   - Manifests JSON : M√©tadonn√©es de chaque run (sources, compteurs, chemins)
   - Hash SHA-256 : D√©duplication + RGPD

4. **Robustesse** :
   - Try/except par source (1 source qui fail ‚â† pipeline qui crash)
   - Retry logic (tenacity) pour APIs
   - Fallback gracieux (continue avec autres sources)

### Diagramme de flux complet

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    üåê SOURCES EXTERNES                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  üìÅ Fichier plat (Kaggle CSV)                                 ‚îÇ
‚îÇ  üóÑÔ∏è Base donn√©es (Kaggle SQLite)                             ‚îÇ
‚îÇ  üåê API (OpenWeatherMap, NewsAPI, RSS)                        ‚îÇ
‚îÇ  üï∏Ô∏è Web Scraping (MonAvisCitoyen, Reddit, etc.)               ‚îÇ
‚îÇ  üìä Big Data (GDELT GKG)                                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    üì• EXTRACT (03_ingest_sources.ipynb)        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚Ä¢ Collecte brute depuis chaque source                         ‚îÇ
‚îÇ  ‚Ä¢ Logger.info("Source X collect√©e")                           ‚îÇ
‚îÇ  ‚Ä¢ Gestion erreurs (try/except + log_error)                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    ‚öôÔ∏è TRANSFORM                                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  1. Calcul hash_fingerprint (SHA-256)                         ‚îÇ
‚îÇ  2. Nettoyage texte (regex, HTML)                              ‚îÇ
‚îÇ  3. Normalisation format ({titre, texte, date, ...})          ‚îÇ
‚îÇ  4. D√©duplication (ON CONFLICT hash_fingerprint)              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                     ‚îÇ                  ‚îÇ                         ‚îÇ
                     ‚ñº                  ‚ñº                         ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   MinIO DataLake  ‚îÇ  ‚îÇ   PostgreSQL      ‚îÇ  ‚îÇ      Logs        ‚îÇ
        ‚îÇ   (Bruts 50%)     ‚îÇ  ‚îÇ   (Structur√©)     ‚îÇ  ‚îÇ    (Audit)       ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ                  ‚îÇ                         ‚îÇ
                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                      ‚îÇ
                                      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    üìä ANALYTICS                                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚Ä¢ CRUD complet (04_crud_tests.ipynb)                         ‚îÇ
‚îÇ  ‚Ä¢ Quality checks (doublons, % NULL)                           ‚îÇ
‚îÇ  ‚Ä¢ KPIs (documents/source, √©v√©nements/th√®me)                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Fonctions helpers (r√©f√©rence `datasens_E1_v2.ipynb`)

**Toutes ces fonctions sont int√©gr√©es dans `03_ingest_sources.ipynb`** :

```python
def ts() -> str:
    """Timestamp UTC ISO compact"""
    return datetime.now(timezone.utc).strftime("%Y%m%dT%H%M%SZ")

def sha256(s: str) -> str:
    """Hash SHA-256 pour d√©duplication"""
    return hashlib.sha256(s.encode("utf-8")).hexdigest()

def get_source_id(conn, nom: str) -> int:
    """R√©cup√®re id_source depuis nom"""
    result = conn.execute(text("SELECT id_source FROM source WHERE nom = :nom"), {"nom": nom}).fetchone()
    return result[0] if result else None

def create_flux(conn, id_source: int, format_type: str = "csv", manifest_uri: str = None) -> int:
    """Cr√©e un flux et retourne id_flux"""
    result = conn.execute(text("""
        INSERT INTO flux (id_source, format, manifest_uri)
        VALUES (:id_source, :format, :manifest_uri)
        RETURNING id_flux
    """), {"id_source": id_source, "format": format_type, "manifest_uri": manifest_uri})
    return result.scalar()

def ensure_territoire(conn, ville: str, code_insee: str = None, lat: float = None, lon: float = None) -> int:
    """Cr√©e ou r√©cup√®re un territoire"""
    result = conn.execute(text("SELECT id_territoire FROM territoire WHERE ville = :ville"), {"ville": ville}).fetchone()
    if result:
        return result[0]
    result = conn.execute(text("""
        INSERT INTO territoire (ville, code_insee, lat, lon)
        VALUES (:ville, :code_insee, :lat, :lon)
        RETURNING id_territoire
    """), {"ville": ville, "code_insee": code_insee, "lat": lat, "lon": lon})
    return result.scalar()

def insert_documents(conn, docs: list) -> int:
    """Insertion batch avec gestion doublons"""
    inserted = 0
    for doc in docs:
        result = conn.execute(text("""
            INSERT INTO document (id_flux, id_territoire, titre, texte, langue, date_publication, hash_fingerprint)
            VALUES (:id_flux, :id_territoire, :titre, :texte, :langue, :date_publication, :hash_fingerprint)
            ON CONFLICT (hash_fingerprint) DO NOTHING
            RETURNING id_doc
        """), doc)
        if result.scalar():
            inserted += 1
    return inserted

def minio_upload(local_path: Path, dest_key: str) -> str:
    """Upload fichier vers MinIO DataLake"""
    ensure_bucket(MINIO_BUCKET)
    minio_client.fput_object(MINIO_BUCKET, dest_key, str(local_path))
    return f"s3://{MINIO_BUCKET}/{dest_key}"
```

---

## üìä Diagrammes et visualisations {#diagrammes}

### Diagramme MCD/MLD simplifi√© (18 tables E1)

```mermaid
erDiagram
    TYPE_DONNEE ||--o{ SOURCE : "a pour"
    SOURCE ||--o{ FLUX : "g√©n√®re"
    FLUX ||--o{ DOCUMENT : "contient"
    TERRITOIRE ||--o{ DOCUMENT : "g√©olocalise"
    TERRITOIRE ||--o{ METEO : "mesure"
    TERRITOIRE ||--o{ INDICATEUR : "agr√®ge"
    THEME ||--o{ EVENEMENT : "classe"
    DOCUMENT ||--o{ DOCUMENT_EVENEMENT : "ref√®re"
    EVENEMENT ||--o{ DOCUMENT_EVENEMENT : "associe"
    TYPE_METEO ||--o{ METEO : "cat√©gorise"
    TYPE_INDICATEUR ||--o{ INDICATEUR : "d√©finit"
    SOURCE_INDICATEUR ||--o{ INDICATEUR : "provoque"
    PIPELINE ||--o{ ETAPE_ETL : "compose"
    QC_RULE ||--o{ QC_RESULT : "valide"
    FLUX ||--o{ QC_RESULT : "teste"
```

### Architecture stockage dual

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  üèõÔ∏è ARCHITECTURE DE STOCKAGE                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

üìä PostgreSQL (Donn√©es Structur√©es)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Tables relationnelles (18 tables Merise)                     ‚îÇ
‚îÇ  ‚Ä¢ Metadata (source, flux, territoire)                          ‚îÇ
‚îÇ  ‚Ä¢ Documents nettoy√©s (hash_fingerprint unique)                ‚îÇ
‚îÇ  ‚Ä¢ Contexte (m√©t√©o, indicateurs)                               ‚îÇ
‚îÇ  ‚Ä¢ √âv√©nements (evenement, document_evenement)                  ‚îÇ
‚îÇ  ‚Ä¢ Gouvernance (pipeline, etape_etl)                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚ñ≤
                        ‚îÇ
                   INSERT
                        ‚îÇ
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ   03_ingest_sources.ipynb                              ‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                   UPLOAD
                        ‚îÇ
                        ‚ñº
‚òÅÔ∏è MinIO (Fichiers Bruts - DataLake)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  s3://datasens-raw/                                            ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ kaggle/                                                   ‚îÇ
‚îÇ  ‚îÇ   ‚îî‚îÄ‚îÄ kaggle_bilingual_YYYYMMDDTHHMMSSZ.csv                ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ api/                                                      ‚îÇ
‚îÇ  ‚îÇ   ‚îú‚îÄ‚îÄ owm/                                                  ‚îÇ
‚îÇ  ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ owm_YYYYMMDDTHHMMSSZ.csv                         ‚îÇ
‚îÇ  ‚îÇ   ‚îî‚îÄ‚îÄ newsapi/                                              ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ scraping/                                                 ‚îÇ
‚îÇ  ‚îÇ   ‚îî‚îÄ‚îÄ multi/scraping_multi_YYYYMMDDTHHMMSSZ.csv            ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ gdelt/                                                    ‚îÇ
‚îÇ  ‚îÇ   ‚îî‚îÄ‚îÄ YYYYMMDDHHMMSS.gkg.csv.zip                          ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ manifests/                                                ‚îÇ
‚îÇ      ‚îî‚îÄ‚îÄ manifest_YYYYMMDDTHHMMSSZ.json                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üõ†Ô∏è Troubleshooting {#troubleshooting}

### Probl√®me 1 : `ModuleNotFoundError`

**Sympt√¥me** :
```
ModuleNotFoundError: No module named 'pandas'
```

**Solution** :
```bash
pip install -r requirements.txt
```

---

### Probl√®me 2 : `OperationalError: could not connect to server`

**Sympt√¥me** :
```
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not connect to server
```

**Causes possibles** :
1. PostgreSQL n'est pas d√©marr√©
2. Mauvais host/port dans `.env`
3. Firewall bloque le port 5432

**Solutions** :

1. **V√©rifier Docker Compose** :
   ```bash
   docker-compose ps
   # Si postgres n'est pas UP, lancer :
   docker-compose up -d postgres
   ```

2. **V√©rifier credentials `.env`** :
   ```bash
   # Windows PowerShell
   Get-Content .env | Select-String "POSTGRES"
   ```

3. **Test connexion manuelle** :
   ```bash
   # Depuis PowerShell
   psql -h localhost -U ds_user -d datasens
   ```

---

### Probl√®me 3 : MinIO non accessible

**Sympt√¥me** :
```
‚ö†Ô∏è MinIO : Erreur connexion
```

**Solutions** :

1. **V√©rifier Docker Compose** :
   ```bash
   docker-compose ps
   # Si minio n'est pas UP, lancer :
   docker-compose up -d minio
   ```

2. **V√©rifier endpoint `.env`** :
   ```bash
   # Doit √™tre : MINIO_ENDPOINT=http://localhost:9000
   ```

3. **Acc√®s console MinIO** :
   - Ouvrir `http://localhost:9001` dans navigateur
   - Login : `miniouser` / `miniosecret`

---

### Probl√®me 4 : Erreur DDL "table already exists"

**Sympt√¥me** :
```
psycopg2.errors.DuplicateTable: relation "document" already exists
```

**Solutions** :

1. **Supprimer tables existantes** (‚ö†Ô∏è perte de donn√©es) :
   ```sql
   DROP TABLE IF EXISTS document CASCADE;
   -- R√©p√©ter pour toutes les tables
   ```

2. **OU utiliser CREATE TABLE IF NOT EXISTS** (d√©j√† dans le code) :
   ```sql
   CREATE TABLE IF NOT EXISTS document (...);
   ```

---

### Probl√®me 5 : Doublons d√©tect√©s lors de l'ingestion

**Sympt√¥me** :
```
0 documents ins√©r√©s (tous des doublons)
```

**Explication** :
- Normal si vous r√©ex√©cutez `03_ingest_sources.ipynb` plusieurs fois
- La contrainte `UNIQUE (hash_fingerprint)` + `ON CONFLICT DO NOTHING` √©vite les doublons

**V√©rification** :
```python
# Dans 04_crud_tests.ipynb, cellule Quality Checks
# V√©rifier qu'il n'y a pas de vrais doublons :
SELECT hash_fingerprint, COUNT(*)
FROM document
GROUP BY hash_fingerprint
HAVING COUNT(*) > 1;
# ‚Üí Doit retourner 0 lignes
```

---

## üìñ Glossaire et concepts {#glossaire}

### Termes techniques expliqu√©s

| Terme | D√©finition simple | Exemple |
|-------|-------------------|---------|
| **ETL** | Extract-Transform-Load : Aspirer, nettoyer, ranger | RSS ‚Üí nettoyage ‚Üí PostgreSQL |
| **MCD/MLD** | Mod√®le Conceptuel/Logique de Donn√©es (Merise) | Diagramme relations entre tables |
| **DataLake** | Stockage fichiers bruts (S3-like) | MinIO : `s3://datasens-raw/kaggle/` |
| **Hash SHA-256** | Empreinte digitale unique (64 caract√®res) | `a3f5c9d2e4b6f8a1c3e5d7b9f2a4c6e8` |
| **CRUD** | Create, Read, Update, Delete (op√©rations de base) | INSERT, SELECT, UPDATE, DELETE |
| **Manifest** | Fichier JSON m√©tadonn√©es d'une collecte | `manifest_20251029T123337Z.json` |
| **Flux** | Enregistrement d'une collecte (table `flux`) | Source + date + format + manifest_uri |
| **Fingerprint** | Hash SHA-256 pour d√©duplication | `hash_fingerprint VARCHAR(64) UNIQUE` |

### Concepts Python avanc√©s

#### Context Manager (`with`)

```python
with engine.connect() as conn:
    result = conn.execute(text("SELECT 1"))
    # Connexion ferm√©e automatiquement ici (m√™me en cas d'erreur)
```

**Avantage** : Gestion automatique ressources (√©vite fuites de connexions)

---

#### Param√®tres nomm√©s SQL (SQLAlchemy)

```python
conn.execute(text("SELECT * FROM document WHERE id = :id"), {"id": 123})
```

**Avantage** : S√©curit√© (√©vite injection SQL) + Lisibilit√©

---

#### ON CONFLICT DO NOTHING

```sql
INSERT INTO document (hash_fingerprint, ...)
VALUES (:hash, ...)
ON CONFLICT (hash_fingerprint) DO NOTHING;
```

**Avantage** : D√©duplication automatique (pas d'erreur si doublon)

---

## ‚úÖ Checklist validation finale

Avant de pr√©senter au jury, v√©rifier :

### Environnement
- [ ] Python 3.11+ install√©
- [ ] Tous les packages (`pip install -r requirements.txt`)
- [ ] Docker Compose lanc√© (`docker-compose up -d`)
- [ ] `.env` configur√© (PostgreSQL, MinIO, API keys)

### Notebooks ex√©cut√©s
- [ ] `01_setup_env.ipynb` : Connexions OK
- [ ] `02_schema_create.ipynb` : 18 tables cr√©√©es
- [ ] `03_ingest_sources.ipynb` : 5 sources ing√©r√©es
- [ ] `04_crud_tests.ipynb` : CRUD d√©montr√©
- [ ] `05_snapshot_and_readme.ipynb` : Exports OK

### Donn√©es
- [ ] Documents en base (v√©rifier `SELECT COUNT(*) FROM document`)
- [ ] Flux cr√©√©s (v√©rifier `SELECT COUNT(*) FROM flux`)
- [ ] Manifests JSON g√©n√©r√©s (`data/raw/manifests/`)
- [ ] Logs g√©n√©r√©s (`logs/collecte_*.log`)

### Git
- [ ] Tag E1 cr√©√© (`git tag E1_REAL_YYYYMMDD`)
- [ ] Commit final effectu√©

---

## üé§ Pr√©sentation jury - Script 2 minutes

> "DataSens E1, c'est un **socle de donn√©es pour entra√Ænement IA** avec 5 notebooks Jupyter structur√©s.
>
> **Notebook 1** : Setup environnement, connexions PostgreSQL + MinIO
>
> **Notebook 2** : Cr√©ation sch√©ma Merise (18 tables relationnelles) avec contraintes et index
>
> **Notebook 3** : Ingestion r√©elle de 5 types de sources :
> - Fichier plat (Kaggle CSV ‚Üí 50% PostgreSQL, 50% MinIO)
> - Base de donn√©es (Kaggle SQLite ‚Üí PostgreSQL)
> - API (OpenWeatherMap ‚Üí m√©t√©o + flux)
> - Web Scraping (MonAvisCitoyen ‚Üí documents)
> - Big Data (GDELT GKG ‚Üí √©v√©nements)
>
> **Notebook 4** : D√©monstration CRUD complet + contr√¥les qualit√©
>
> **Notebook 5** : Bilan E1, exports DDL/CSV, roadmap E2/E3
>
> **R√©sultat** : Pipeline ETL reproductible, tra√ßabilit√© compl√®te (logs, manifests, Git), architecture scalable pour industrialisation dataset √† grande √©chelle."

---

## üìö Ressources compl√©mentaires

### Documentation officielle
- **SQLAlchemy** : https://docs.sqlalchemy.org/
- **Pandas** : https://pandas.pydata.org/docs/
- **MinIO** : https://min.io/docs/
- **PostgreSQL** : https://www.postgresql.org/docs/

### Projets de r√©f√©rence
- `notebooks/datasens_E1_v2.ipynb` : Architecture pipeline compl√®te
- `docs/ARCHITECTURE_ETL.md` : Architecture ETL d√©taill√©e
- `docs/ARCHITECTURE_PIPELINE_E1.md` : Alignement pipeline

---

**üéì Bonne chance pour la pr√©sentation au jury !** üöÄ
