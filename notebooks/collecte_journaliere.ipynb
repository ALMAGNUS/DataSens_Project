{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "040fcba6",
   "metadata": {},
   "source": [
    "# üîÑ DataSens - Collecte Journali√®re Automatis√©e\n",
    "\n",
    "**Objectif** : Orchestrer la collecte quotidienne des 5 sources de donn√©es en production\n",
    "\n",
    "**Strat√©gie d'enrichissement continu** :\n",
    "- ‚è∞ Ex√©cution planifi√©e (CRON, Prefect, Airflow)\n",
    "- üîí D√©duplication automatique (hash SHA256)\n",
    "- üìä Monitoring et logs de collecte\n",
    "- üîÑ Reprise sur erreur (retry automatique)\n",
    "- üíæ Backup PostgreSQL quotidien\n",
    "\n",
    "**Sources collect√©es** :\n",
    "1. Kaggle CSV (hebdomadaire)\n",
    "2. OpenWeatherMap API (4x/jour)\n",
    "3. RSS Multi-Sources (toutes les heures)\n",
    "4. Web Scraping (quotidien)\n",
    "5. GDELT Big Data (toutes les 15 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e486d211",
   "metadata": {},
   "source": [
    "## üì¶ Imports et Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45f648e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration charg√©e\n",
      "üìÇ ROOT : c:\\Users\\Utilisateur\\Desktop\\Datasens_Project\n",
      "üóÑÔ∏è  PostgreSQL : ds_user@localhost:5432/datasens\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Environnement\n",
    "ROOT = Path.cwd().parent\n",
    "load_dotenv(ROOT / \".env\")\n",
    "\n",
    "# PostgreSQL\n",
    "PG_USER = os.getenv(\"POSTGRES_USER\", \"ds_user\")\n",
    "PG_PASS = os.getenv(\"POSTGRES_PASSWORD\", \"ds_pass\")\n",
    "PG_HOST = os.getenv(\"POSTGRES_HOST\", \"localhost\")\n",
    "PG_PORT = int(os.getenv(\"POSTGRES_PORT\", 5432))\n",
    "PG_DB = os.getenv(\"POSTGRES_DB\", \"datasens\")\n",
    "\n",
    "print(\"‚úÖ Configuration charg√©e\")\n",
    "print(f\"üìÇ ROOT : {ROOT}\")\n",
    "print(f\"üóÑÔ∏è  PostgreSQL : {PG_USER}@{PG_HOST}:{PG_PORT}/{PG_DB}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc0083a",
   "metadata": {},
   "source": [
    "## üìù Syst√®me de versioning et logs\n",
    "\n",
    "Tra√ßabilit√© compl√®te de chaque collecte journali√®re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cec23383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Log : COLLECTE_JOURNALIERE_INIT ‚Äî D√©marrage script collecte quotidienne\n",
      "\n",
      "üîß Fonctions de versioning charg√©es\n"
     ]
    }
   ],
   "source": [
    "VERSION_FILE = ROOT / \"README_VERSIONNING.md\"\n",
    "VERSIONS_DIR = ROOT / \"datasens\" / \"versions\"\n",
    "VERSIONS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def log_version(action: str, details: str = \"\"):\n",
    "    \"\"\"Logger simple : timestamp + action + d√©tails ‚Üí README_VERSIONNING.md\"\"\"\n",
    "    now = dt.datetime.now(dt.UTC).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    entry = f\"- **{now} UTC** | `{action}` | {details}\\n\"\n",
    "    \n",
    "    with open(VERSION_FILE, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(entry)\n",
    "    \n",
    "    print(f\"üìù Log : {action} ‚Äî {details}\")\n",
    "\n",
    "def save_postgres_snapshot(note=\"Snapshot PostgreSQL quotidien\"):\n",
    "    \"\"\"Cr√©e un dump PostgreSQL horodat√© via Docker\"\"\"\n",
    "    timestamp = dt.datetime.now(dt.UTC).strftime(\"%Y%m%d_%H%M%S\")\n",
    "    dump_name = f\"datasens_pg_v{timestamp}.sql\"\n",
    "    dump_path = VERSIONS_DIR / dump_name\n",
    "    \n",
    "    # D√©tecter automatiquement le conteneur PostgreSQL\n",
    "    try:\n",
    "        result_ps = subprocess.run(\n",
    "            [\"docker\", \"ps\", \"--filter\", \"name=postgres\", \"--format\", \"{{.Names}}\"],\n",
    "            capture_output=True, text=True, check=True\n",
    "        )\n",
    "        container_name = result_ps.stdout.strip().split('\\n')[0]\n",
    "        \n",
    "        if not container_name:\n",
    "            raise Exception(\"Aucun conteneur PostgreSQL trouv√©\")\n",
    "        \n",
    "        print(f\"üê≥ Conteneur d√©tect√© : {container_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Impossible de d√©tecter le conteneur : {e}\")\n",
    "        print(\"   Tentative avec nom par d√©faut 'datasens-postgres'\")\n",
    "        container_name = \"datasens-postgres\"\n",
    "    \n",
    "    # Utiliser Docker pour pg_dump\n",
    "    cmd = [\n",
    "        \"docker\", \"exec\",\n",
    "        container_name,\n",
    "        \"pg_dump\",\n",
    "        \"-U\", PG_USER,\n",
    "        PG_DB\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # Ex√©cuter la commande avec encodage UTF-8 pour g√©rer les caract√®res fran√ßais\n",
    "        result = subprocess.run(cmd, check=True, capture_output=True, text=True, encoding='utf-8', errors='replace')\n",
    "        \n",
    "        # √âcrire le dump dans le fichier avec encodage UTF-8\n",
    "        with open(dump_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(result.stdout)\n",
    "        \n",
    "        log_version(\"PG_SNAPSHOT\", f\"{dump_name} ‚Äî {note}\")\n",
    "        print(f\"‚úÖ Snapshot PostgreSQL cr√©√© : {dump_name}\")\n",
    "        print(f\"   Taille : {dump_path.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "        return dump_path\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ö†Ô∏è Docker non trouv√©. Assurez-vous que Docker Desktop est d√©marr√©.\")\n",
    "        log_version(\"PG_SNAPSHOT_FAIL\", \"Docker manquant ou non d√©marr√©\")\n",
    "        return None\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ùå Erreur pg_dump via Docker : {e.stderr}\")\n",
    "        print(f\"   Conteneur utilis√© : {container_name}\")\n",
    "        print(\"   V√©rifiez que le conteneur PostgreSQL est running avec : docker ps\")\n",
    "        log_version(\"PG_SNAPSHOT_ERROR\", str(e.stderr)[:100])\n",
    "        return None\n",
    "\n",
    "# Initialiser le fichier de versioning s'il n'existe pas\n",
    "if not VERSION_FILE.exists():\n",
    "    with open(VERSION_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"# üìò Historique des versions DataSens\\n\\n\")\n",
    "    print(f\"‚úÖ Fichier de versioning cr√©√© : {VERSION_FILE}\")\n",
    "\n",
    "log_version(\"COLLECTE_JOURNALIERE_INIT\", \"D√©marrage script collecte quotidienne\")\n",
    "print(\"\\nüîß Fonctions de versioning charg√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6eea69",
   "metadata": {},
   "source": [
    "## üìä Statistiques pr√©-collecte\n",
    "\n",
    "√âtat de la base avant la collecte journali√®re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5e0a732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä √âTAT PR√â-COLLECTE\n",
      "============================================================\n",
      "Total documents : 25,047\n",
      "\n",
      "R√©partition par source (sources actives uniquement) :\n",
      "                                                        nom  nb_docs\n",
      "                                                 Kaggle CSV    24683\n",
      "                                 Web Scraping Multi-Sources      265\n",
      "Flux RSS Multi-Sources (Franceinfo + 20 Minutes + Le Monde)       99\n",
      "\n",
      "üìà 3 sources actives | 25,047 documents au total\n",
      "‚è∞ Timestamp : 2025-10-28T13:49:16.265701+00:00Z\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "PG_URL = f\"postgresql+psycopg2://{PG_USER}:{PG_PASS}@{PG_HOST}:{PG_PORT}/{PG_DB}\"\n",
    "engine = create_engine(PG_URL, future=True)\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    # Total documents\n",
    "    total_docs = conn.execute(text(\"SELECT COUNT(*) FROM document\")).scalar()\n",
    "    \n",
    "    # Par source (uniquement les sources actives avec documents)\n",
    "    query_sources = text(\"\"\"\n",
    "        SELECT s.nom, COUNT(d.id_doc) as nb_docs\n",
    "        FROM source s\n",
    "        LEFT JOIN flux f ON s.id_source = f.id_source\n",
    "        LEFT JOIN document d ON f.id_flux = d.id_flux\n",
    "        GROUP BY s.id_source, s.nom\n",
    "        HAVING COUNT(d.id_doc) > 0\n",
    "        ORDER BY nb_docs DESC\n",
    "    \"\"\")\n",
    "    df_sources = pd.read_sql_query(query_sources, conn)\n",
    "\n",
    "print(f\"üìä √âTAT PR√â-COLLECTE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total documents : {total_docs:,}\")\n",
    "print(f\"\\nR√©partition par source (sources actives uniquement) :\")\n",
    "print(df_sources.to_string(index=False))\n",
    "print(f\"\\nüìà {len(df_sources)} sources actives | {total_docs:,} documents au total\")\n",
    "print(f\"‚è∞ Timestamp : {dt.datetime.now(dt.UTC).isoformat()}Z\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffdaf72",
   "metadata": {},
   "source": [
    "## üîÑ Planification des collectes\n",
    "\n",
    "### Strat√©gie de fr√©quence par source\n",
    "\n",
    "| Source | Fr√©quence | Justification |\n",
    "|--------|-----------|---------------|\n",
    "| **Kaggle CSV** | Hebdomadaire | Datasets statiques peu mis √† jour |\n",
    "| **OpenWeatherMap** | 4x/jour (6h, 12h, 18h, 00h) | M√©t√©o temps r√©el, 4 relev√©s quotidiens suffisants |\n",
    "| **RSS Multi-Sources** | Toutes les heures | Actualit√©s changeantes, rythme m√©diatique |\n",
    "| **Web Scraping** | Quotidien (2h du matin) | √âviter surcharge serveurs, respect rate limits |\n",
    "| **GDELT Big Data** | Toutes les 15 min | Flux temps r√©el, events mondiaux |\n",
    "\n",
    "### Impl√©mentation avec Prefect (recommand√©)\n",
    "\n",
    "```python\n",
    "# Exemple avec Prefect pour orchestration\n",
    "from prefect import flow, task\n",
    "from prefect.schedules import CronSchedule\n",
    "\n",
    "@task(retries=3, retry_delay_seconds=300)\n",
    "def collect_rss():\n",
    "    # Code collecte RSS\n",
    "    pass\n",
    "\n",
    "@task(retries=3)\n",
    "def collect_owm():\n",
    "    # Code collecte OpenWeatherMap\n",
    "    pass\n",
    "\n",
    "@flow(name=\"datasens-daily-collection\")\n",
    "def daily_collection_flow():\n",
    "    collect_rss()\n",
    "    collect_owm()\n",
    "    # ... autres sources\n",
    "\n",
    "# Planification CRON\n",
    "# RSS : 0 * * * * (toutes les heures)\n",
    "# OWM : 0 6,12,18,0 * * * (6h, 12h, 18h, 00h)\n",
    "# Scraping : 0 2 * * * (2h du matin)\n",
    "```\n",
    "\n",
    "### Alternative : Airflow DAG\n",
    "\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import timedelta\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'datasens',\n",
    "    'retries': 3,\n",
    "    'retry_delay': timedelta(minutes=5)\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    'datasens_daily_collection',\n",
    "    default_args=default_args,\n",
    "    schedule_interval='0 2 * * *',  # 2h du matin\n",
    "    catchup=False\n",
    ")\n",
    "\n",
    "task_rss = PythonOperator(\n",
    "    task_id='collect_rss',\n",
    "    python_callable=collect_rss_function,\n",
    "    dag=dag\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ae9764",
   "metadata": {},
   "source": [
    "## üéØ D√©duplication intelligente\n",
    "\n",
    "M√©canisme pour √©viter les doublons lors des collectes r√©p√©t√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9b00a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ D√©duplication automatique activ√©e\n",
      "   M√©thode : SHA256 hash_fingerprint + UNIQUE constraint\n",
      "   Comportement : Les doublons sont silencieusement ignor√©s (ON CONFLICT DO NOTHING)\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "def sha256(text: str) -> str:\n",
    "    \"\"\"Hash SHA256 pour empreinte unique de document\"\"\"\n",
    "    return hashlib.sha256(text.encode('utf-8')).hexdigest()\n",
    "\n",
    "# La d√©duplication se fait automatiquement gr√¢ce √† :\n",
    "# 1. Colonne hash_fingerprint UNIQUE dans PostgreSQL\n",
    "# 2. Clause ON CONFLICT DO NOTHING dans insert_documents()\n",
    "\n",
    "print(\"‚úÖ D√©duplication automatique activ√©e\")\n",
    "print(\"   M√©thode : SHA256 hash_fingerprint + UNIQUE constraint\")\n",
    "print(\"   Comportement : Les doublons sont silencieusement ignor√©s (ON CONFLICT DO NOTHING)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8123f45d",
   "metadata": {},
   "source": [
    "## üìà Monitoring et alertes\n",
    "\n",
    "Suivi de la sant√© des collectes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc4f15a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üè• SANT√â DES COLLECTES\n",
      "================================================================================\n",
      "Web Scraping Multi-Sources                         |    265 docs | ‚úÖ OK (2025-10-28 13:19:43.201839)\n",
      "Flux RSS Multi-Sources (Franceinfo + 20 Minutes + Le Monde) |     99 docs | ‚úÖ OK (2025-10-28 13:19:13.302066)\n",
      "OpenWeatherMap                                     |      0 docs | ‚úÖ OK (2025-10-28 13:19:09.552343)\n",
      "Kaggle CSV                                         | 24,683 docs | ‚úÖ OK (2025-10-28 13:18:30.750655)\n",
      "\n",
      "================================================================================\n",
      "üìä R√©sum√© : 4 sources actives (<24h) | 0 sources obsol√®tes (>24h)\n"
     ]
    }
   ],
   "source": [
    "def check_collection_health():\n",
    "    \"\"\"V√©rifie que chaque source a collect√© des donn√©es r√©cemment (< 24h)\"\"\"\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        query = text(\"\"\"\n",
    "            SELECT \n",
    "                s.nom,\n",
    "                COUNT(d.id_doc) as total_docs,\n",
    "                MAX(f.date_collecte) as derniere_collecte,\n",
    "                NOW() - MAX(f.date_collecte) as age_dernier_flux\n",
    "            FROM source s\n",
    "            LEFT JOIN flux f ON s.id_source = f.id_source\n",
    "            LEFT JOIN document d ON f.id_flux = d.id_flux\n",
    "            GROUP BY s.id_source, s.nom\n",
    "            HAVING COUNT(d.id_doc) > 0 OR MAX(f.date_collecte) IS NOT NULL\n",
    "            ORDER BY derniere_collecte DESC NULLS LAST\n",
    "        \"\"\")\n",
    "        df = pd.read_sql_query(query, conn)\n",
    "    \n",
    "    print(\"üè• SANT√â DES COLLECTES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    sources_actives = 0\n",
    "    sources_obsoletes = 0\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        nom = row['nom']\n",
    "        total = row['total_docs']\n",
    "        derniere = row['derniere_collecte']\n",
    "        \n",
    "        if pd.isna(derniere):\n",
    "            status = \"‚ùå JAMAIS COLLECT√â\"\n",
    "        elif row['age_dernier_flux'].total_seconds() > 86400:  # > 24h\n",
    "            status = f\"‚ö†Ô∏è OBSOL√àTE ({derniere})\"\n",
    "            sources_obsoletes += 1\n",
    "        else:\n",
    "            status = f\"‚úÖ OK ({derniere})\"\n",
    "            sources_actives += 1\n",
    "        \n",
    "        print(f\"{nom:50s} | {total:6,} docs | {status}\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üìä R√©sum√© : {sources_actives} sources actives (<24h) | {sources_obsoletes} sources obsol√®tes (>24h)\")\n",
    "    \n",
    "    # Avertissements si n√©cessaire\n",
    "    if sources_obsoletes > 0:\n",
    "        print(f\"‚ö†Ô∏è  {sources_obsoletes} source(s) n'ont pas collect√© depuis >24h - V√©rifier les planifications\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "health_df = check_collection_health()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfadf62",
   "metadata": {},
   "source": [
    "## üíæ Backup automatique quotidien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2ca8c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üê≥ Conteneur d√©tect√© : datasens-postgres\n",
      "üìù Log : PG_SNAPSHOT ‚Äî datasens_pg_v20251028_135152.sql ‚Äî Backup quotidien automatique\n",
      "‚úÖ Snapshot PostgreSQL cr√©√© : datasens_pg_v20251028_135152.sql\n",
      "   Taille : 5.81 MB\n",
      "\n",
      "‚úÖ Backup PostgreSQL : c:\\Users\\Utilisateur\\Desktop\\Datasens_Project\\datasens\\versions\\datasens_pg_v20251028_135152.sql\n",
      "   Taille : 5949.72 Ko\n",
      "üìù Log : PG_SNAPSHOT ‚Äî datasens_pg_v20251028_135152.sql ‚Äî Backup quotidien automatique\n",
      "‚úÖ Snapshot PostgreSQL cr√©√© : datasens_pg_v20251028_135152.sql\n",
      "   Taille : 5.81 MB\n",
      "\n",
      "‚úÖ Backup PostgreSQL : c:\\Users\\Utilisateur\\Desktop\\Datasens_Project\\datasens\\versions\\datasens_pg_v20251028_135152.sql\n",
      "   Taille : 5949.72 Ko\n"
     ]
    }
   ],
   "source": [
    "# Cr√©er le snapshot PostgreSQL quotidien\n",
    "snapshot_path = save_postgres_snapshot(\"Backup quotidien automatique\")\n",
    "\n",
    "if snapshot_path:\n",
    "    print(f\"\\n‚úÖ Backup PostgreSQL : {snapshot_path}\")\n",
    "    print(f\"   Taille : {snapshot_path.stat().st_size / 1024:.2f} Ko\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Snapshot non cr√©√© automatiquement.\")\n",
    "    print(\"   Commande manuelle (dans le terminal) :\")\n",
    "    print(f\"   docker exec datasens-postgres pg_dump -U {PG_USER} {PG_DB} > backup.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aceb3292",
   "metadata": {},
   "source": [
    "## üìä Rapport final post-collecte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b7f09af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä RAPPORT POST-COLLECTE\n",
      "============================================================\n",
      "Total documents : 25,047 (avant: 25,047)\n",
      "Nouveaux documents (24h) : 25,047\n",
      "Croissance : +0 documents\n",
      "\n",
      "‚è∞ Fin collecte : 2025-10-28T13:52:00.552932+00:00Z\n",
      "üìù Log : COLLECTE_JOURNALIERE_FIN ‚Äî +25047 nouveaux documents en 24h\n"
     ]
    }
   ],
   "source": [
    "with engine.connect() as conn:\n",
    "    # Total documents apr√®s collecte\n",
    "    total_docs_after = conn.execute(text(\"SELECT COUNT(*) FROM document\")).scalar()\n",
    "    \n",
    "    # Nouveaux documents (derni√®res 24h)\n",
    "    query_new = text(\"\"\"\n",
    "        SELECT COUNT(*) \n",
    "        FROM document d\n",
    "        JOIN flux f ON d.id_flux = f.id_flux\n",
    "        WHERE f.date_collecte >= NOW() - INTERVAL '24 hours'\n",
    "    \"\"\")\n",
    "    new_docs = conn.execute(query_new).scalar()\n",
    "\n",
    "print(f\"\\nüìä RAPPORT POST-COLLECTE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total documents : {total_docs_after:,} (avant: {total_docs:,})\")\n",
    "print(f\"Nouveaux documents (24h) : {new_docs:,}\")\n",
    "print(f\"Croissance : +{total_docs_after - total_docs:,} documents\")\n",
    "print(f\"\\n‚è∞ Fin collecte : {dt.datetime.now(dt.UTC).isoformat()}Z\")\n",
    "\n",
    "log_version(\"COLLECTE_JOURNALIERE_FIN\", f\"+{new_docs} nouveaux documents en 24h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677f0461",
   "metadata": {},
   "source": [
    "## üöÄ D√©ploiement en production - GitHub Actions (OPTION RETENUE)\n",
    "\n",
    "### ‚úÖ Configuration GitHub Actions d√©ploy√©e\n",
    "\n",
    "Le fichier `.github/workflows/daily-collection.yml` a √©t√© cr√©√© avec :\n",
    "\n",
    "**Fonctionnalit√©s** :\n",
    "- üïê Ex√©cution automatique quotidienne √† 2h UTC (CRON)\n",
    "- üñ±Ô∏è Ex√©cution manuelle possible (workflow_dispatch)\n",
    "- üê≥ Services Docker int√©gr√©s (PostgreSQL, Redis, MinIO)\n",
    "- üìä G√©n√©ration de rapports HTML automatique\n",
    "- üíæ Sauvegarde des artifacts pendant 30 jours\n",
    "- üö® Notifications en cas d'√©chec\n",
    "- üìà Extraction automatique des statistiques\n",
    "\n",
    "**Services d√©marr√©s automatiquement** :\n",
    "- PostgreSQL 15-alpine (port 5432)\n",
    "- Redis Alpine (port 6379)\n",
    "- MinIO (ports 9000, 9001)\n",
    "\n",
    "### üîê Secrets GitHub √† configurer\n",
    "\n",
    "Allez dans **Settings ‚Üí Secrets and variables ‚Üí Actions ‚Üí New repository secret** :\n",
    "\n",
    "```\n",
    "KAGGLE_USERNAME=votre_username_kaggle\n",
    "KAGGLE_KEY=votre_cle_api_kaggle\n",
    "OWM_API_KEY=votre_cle_openweathermap\n",
    "```\n",
    "\n",
    "### üìù √âtapes de d√©ploiement\n",
    "\n",
    "1. **Pousser le code sur GitHub** :\n",
    "   ```bash\n",
    "   git add .github/workflows/daily-collection.yml\n",
    "   git add notebooks/collecte_journaliere.ipynb\n",
    "   git commit -m \"üöÄ Add GitHub Actions daily collection workflow\"\n",
    "   git push origin main\n",
    "   ```\n",
    "\n",
    "2. **Configurer les secrets** (voir ci-dessus)\n",
    "\n",
    "3. **Tester l'ex√©cution manuelle** :\n",
    "   - Aller dans l'onglet **Actions** du repository\n",
    "   - S√©lectionner le workflow \"üìä DataSens - Collecte Quotidienne Automatis√©e\"\n",
    "   - Cliquer sur **Run workflow** ‚Üí **Run workflow**\n",
    "\n",
    "4. **V√©rifier les artifacts** :\n",
    "   - Apr√®s ex√©cution, t√©l√©charger `collection-report-XXX.zip`\n",
    "   - Contient : notebooks ex√©cut√©s (.ipynb), rapports HTML, dumps SQL\n",
    "\n",
    "### üïê Planification CRON\n",
    "\n",
    "```yaml\n",
    "schedule:\n",
    "  - cron: '0 2 * * *'  # 2h UTC = 3h Paris (hiver) / 4h Paris (√©t√©)\n",
    "```\n",
    "\n",
    "**Modifier la fr√©quence** (si n√©cessaire) :\n",
    "- Toutes les 6h : `'0 */6 * * *'`\n",
    "- Deux fois par jour (2h et 14h) : `'0 2,14 * * *'`\n",
    "- Toutes les heures : `'0 * * * *'`\n",
    "\n",
    "### üìä Monitoring des ex√©cutions\n",
    "\n",
    "- **Statut temps r√©el** : Onglet Actions ‚Üí dernier run\n",
    "- **Logs d√©taill√©s** : Cliquer sur un job ‚Üí voir les steps\n",
    "- **Rapports HTML** : Artifacts ‚Üí t√©l√©charger ‚Üí ouvrir .html\n",
    "- **Historique** : Actions ‚Üí filtrer par statut (success/failure)\n",
    "\n",
    "### üéØ Avantages GitHub Actions vs autres solutions\n",
    "\n",
    "| Crit√®re | GitHub Actions | CRON local | Windows Task Scheduler |\n",
    "|---------|----------------|------------|------------------------|\n",
    "| **Infrastructure** | ‚òÅÔ∏è Cloud gratuit | üíª Serveur local | üíª Machine locale |\n",
    "| **Co√ªt** | Gratuit (2000 min/mois) | Co√ªt serveur | Gratuit |\n",
    "| **Fiabilit√©** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê |\n",
    "| **Logs** | Interface web | Fichiers logs | Observateur d'√©v√©nements |\n",
    "| **Artifacts** | Stockage 30j | Manuel | Manuel |\n",
    "| **Rapports HTML** | Auto-g√©n√©r√© | √Ä coder | √Ä coder |\n",
    "| **Notifications** | Int√©gr√© | √Ä configurer | √Ä configurer |\n",
    "| **Services Docker** | Int√©gr√© | √Ä installer | √Ä installer |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
