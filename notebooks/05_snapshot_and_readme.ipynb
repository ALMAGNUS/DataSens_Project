{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdd5d970",
   "metadata": {},
   "source": [
    "# üì∏ DataSens E1 ‚Äî Notebook 5 : Snapshot et README\n",
    "\n",
    "**üéØ Objectif** : Cr√©er un bilan E1, exporter le DDL/CSV, cr√©er un tag Git et d√©finir la roadmap E2/E3\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Contenu de ce notebook\n",
    "\n",
    "1. **Bilan E1** : Ce qui est fait / √† faire\n",
    "2. **Export DDL** : Sauvegarde du sch√©ma SQL dans `docs/e1_schema.sql`\n",
    "3. **Export CSV** : Snapshots des donn√©es dans `data/gold/`\n",
    "4. **Tag Git** : Cr√©ation du tag `E1_REAL_YYYYMMDD`\n",
    "5. **Roadmap E2/E3** : Prochaines √©tapes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc2a035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "import os\n",
    "import subprocess\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == \"notebooks\" else NOTEBOOK_DIR\n",
    "load_dotenv(PROJECT_ROOT / \".env\")\n",
    "\n",
    "PG_HOST = os.getenv(\"POSTGRES_HOST\", \"localhost\")\n",
    "PG_PORT = int(os.getenv(\"POSTGRES_PORT\", \"5432\"))\n",
    "PG_DB = os.getenv(\"POSTGRES_DB\", \"datasens\")\n",
    "PG_USER = os.getenv(\"POSTGRES_USER\", \"ds_user\")\n",
    "PG_PASS = os.getenv(\"POSTGRES_PASS\", \"ds_pass\")\n",
    "\n",
    "PG_URL = f\"postgresql+psycopg2://{PG_USER}:{PG_PASS}@{PG_HOST}:{PG_PORT}/{PG_DB}\"\n",
    "engine = create_engine(PG_URL, future=True)\n",
    "\n",
    "print(\"‚úÖ Configuration charg√©e\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc869f81",
   "metadata": {},
   "source": [
    "## üìä Bilan E1 : Ce qui est fait / √† faire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed22883",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä BILAN E1\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    stats = {\n",
    "        \"tables\": conn.execute(text(\"SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = 'public'\")).scalar(),\n",
    "        \"documents\": conn.execute(text(\"SELECT COUNT(*) FROM document\")).scalar(),\n",
    "        \"flux\": conn.execute(text(\"SELECT COUNT(*) FROM flux\")).scalar(),\n",
    "        \"sources\": conn.execute(text(\"SELECT COUNT(*) FROM source\")).scalar(),\n",
    "        \"meteo\": conn.execute(text(\"SELECT COUNT(*) FROM meteo\")).scalar(),\n",
    "        \"evenements\": conn.execute(text(\"SELECT COUNT(*) FROM evenement\")).scalar(),\n",
    "    }\n",
    "\n",
    "print(\"\\n‚úÖ R√©alis√© :\")\n",
    "print(f\"   ‚Ä¢ {stats['tables']} tables PostgreSQL cr√©√©es (sch√©ma Merise)\")\n",
    "print(f\"   ‚Ä¢ {stats['sources']} sources configur√©es\")\n",
    "print(f\"   ‚Ä¢ {stats['flux']} flux de collecte\")\n",
    "print(f\"   ‚Ä¢ {stats['documents']} documents collect√©s\")\n",
    "print(f\"   ‚Ä¢ {stats['meteo']} relev√©s m√©t√©o\")\n",
    "print(f\"   ‚Ä¢ {stats['evenements']} √©v√©nements\")\n",
    "print(\"\\n‚úÖ 5 types de sources ing√©r√©es :\")\n",
    "print(\"   1. Fichier plat CSV (Kaggle)\")\n",
    "print(\"   2. Base de donn√©es (SQLite ‚Üí Postgres)\")\n",
    "print(\"   3. API (OpenWeatherMap)\")\n",
    "print(\"   4. Web Scraping (MonAvisCitoyen)\")\n",
    "print(\"   5. Big Data (GDELT GKG)\")\n",
    "print(\"\\nüìã √Ä faire ensuite (E2/E3) :\")\n",
    "print(\"   ‚Ä¢ Enrichissement IA (NLP, sentiment analysis)\")\n",
    "print(\"   ‚Ä¢ Dashboard Power BI\")\n",
    "print(\"   ‚Ä¢ Orchestration Prefect/Airflow\")\n",
    "print(\"   ‚Ä¢ Tests automatis√©s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c78b3b",
   "metadata": {},
   "source": [
    "## üíæ Export DDL : Sauvegarde du sch√©ma SQL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade89636",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üíæ Export DDL PostgreSQL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Export du sch√©ma complet\n",
    "with engine.connect() as conn:\n",
    "    schema_query = \"\"\"\n",
    "    SELECT\n",
    "        'CREATE TABLE ' || table_name || ' (' || E'\\\\n' ||\n",
    "        string_agg(\n",
    "            column_name || ' ' ||\n",
    "            CASE\n",
    "                WHEN data_type = 'integer' THEN 'INTEGER'\n",
    "                WHEN data_type = 'bigint' THEN 'BIGINT'\n",
    "                WHEN data_type = 'text' THEN 'TEXT'\n",
    "                WHEN data_type = 'character varying' THEN 'VARCHAR(' || character_maximum_length || ')'\n",
    "                WHEN data_type = 'timestamp without time zone' THEN 'TIMESTAMP'\n",
    "                WHEN data_type = 'real' THEN 'FLOAT'\n",
    "                ELSE data_type\n",
    "            END ||\n",
    "            CASE WHEN is_nullable = 'NO' THEN ' NOT NULL' ELSE '' END,\n",
    "            ',' || E'\\\\n    '\n",
    "            ORDER BY ordinal_position\n",
    "        ) || E'\\\\n);'\n",
    "        as ddl\n",
    "    FROM information_schema.columns\n",
    "    WHERE table_schema = 'public'\n",
    "    GROUP BY table_name;\n",
    "    \"\"\"\n",
    "\n",
    "    # Solution simplifi√©e : utiliser pg_dump ou exporter manuellement\n",
    "    print(\"üìù G√©n√©ration du sch√©ma SQL...\")\n",
    "\n",
    "    # Cr√©er le dossier docs s'il n'existe pas\n",
    "    docs_dir = PROJECT_ROOT / \"docs\"\n",
    "    docs_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Export simplifi√© (pour un export complet, utiliser pg_dump)\n",
    "    schema_export = f\"\"\"\n",
    "-- DataSens E1 - Sch√©ma PostgreSQL\n",
    "-- Export g√©n√©r√© le {datetime.now(timezone.utc).isoformat()}\n",
    "-- 18 tables Merise\n",
    "\n",
    "-- Note: Pour un export complet, utiliser:\n",
    "-- pg_dump -h {PG_HOST} -U {PG_USER} -d {PG_DB} --schema-only > docs/e1_schema.sql\n",
    "\"\"\"\n",
    "\n",
    "    schema_file = docs_dir / \"e1_schema.sql\"\n",
    "    schema_file.write_text(schema_export, encoding=\"utf-8\")\n",
    "\n",
    "    print(f\"‚úÖ Sch√©ma export√© : {schema_file}\")\n",
    "    print(\"   üí° Pour un export complet, ex√©cutez:\")\n",
    "    print(f\"      pg_dump -h {PG_HOST} -U {PG_USER} -d {PG_DB} --schema-only > docs/e1_schema.sql\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb68b15a",
   "metadata": {},
   "source": [
    "## üì§ Export CSV : Snapshots des donn√©es (data/gold/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8658df06",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üì§ Export CSV - Snapshots data/gold/\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "gold_dir = PROJECT_ROOT / \"data\" / \"gold\"\n",
    "gold_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Exporter quelques tables principales\n",
    "tables_to_export = [\"document\", \"source\", \"flux\", \"territoire\", \"meteo\"]\n",
    "\n",
    "exported = []\n",
    "for table in tables_to_export:\n",
    "    try:\n",
    "        df = pd.read_sql(f\"SELECT * FROM {table} LIMIT 1000\", engine)  # Limite pour d√©mo\n",
    "        if len(df) > 0:\n",
    "            csv_path = gold_dir / f\"{table}_{timestamp}.csv\"\n",
    "            df.to_csv(csv_path, index=False)\n",
    "            exported.append(f\"   ‚úÖ {table}: {len(df)} lignes ‚Üí {csv_path.name}\")\n",
    "    except Exception as e:\n",
    "        exported.append(f\"   ‚ö†Ô∏è {table}: Erreur - {e}\")\n",
    "\n",
    "print(\"\\nüìä Exports CSV :\")\n",
    "for item in exported:\n",
    "    print(item)\n",
    "\n",
    "print(f\"\\n‚úÖ Snapshots sauvegard√©s dans data/gold/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b309c109",
   "metadata": {},
   "source": [
    "## üè∑Ô∏è Cr√©ation du tag Git : E1_REAL_YYYYMMDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfb657b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üè∑Ô∏è Cr√©ation tag Git\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "tag_name = f\"E1_REAL_{datetime.now(timezone.utc).strftime('%Y%m%d')}\"\n",
    "\n",
    "git_dir = PROJECT_ROOT / \".git\"\n",
    "if git_dir.exists():\n",
    "    try:\n",
    "        # V√©rifier si le tag existe d√©j√†\n",
    "        result = subprocess.run(\n",
    "            [\"git\", \"tag\", \"-l\", tag_name],\n",
    "            cwd=PROJECT_ROOT,\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "\n",
    "        if tag_name in result.stdout:\n",
    "            print(f\"‚ö†Ô∏è Tag {tag_name} existe d√©j√†\")\n",
    "        else:\n",
    "            # Cr√©er le tag\n",
    "            subprocess.run(\n",
    "                [\"git\", \"tag\", \"-a\", tag_name, \"-m\", f\"DataSens E1 complet - {tag_name}\"],\n",
    "                cwd=PROJECT_ROOT,\n",
    "                check=True\n",
    "            )\n",
    "            print(f\"‚úÖ Tag Git cr√©√© : {tag_name}\")\n",
    "            print(\"   üí° Pour pousser le tag: git push origin {tag_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur cr√©ation tag : {e}\")\n",
    "        print(f\"   üí° Cr√©ation manuelle: git tag -a {tag_name} -m 'DataSens E1'\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è D√©p√¥t Git non initialis√©\")\n",
    "    print(f\"   üí° Tag sugg√©r√©: {tag_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c23aea",
   "metadata": {},
   "source": [
    "## üó∫Ô∏è Roadmap E2/E3\n",
    "\n",
    "Planification des prochaines √©tapes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b92303",
   "metadata": {},
   "source": [
    "### üìã E2 - Enrichissement IA\n",
    "\n",
    "- **Annotation automatique** : Sentiment analysis (FlauBERT, CamemBERT)\n",
    "- **Extraction entit√©s nomm√©es** : spaCy NER (personnes, organisations, lieux)\n",
    "- **Embeddings vectoriels** : sentence-transformers pour recherche s√©mantique\n",
    "- **Classification th√©matique** : ML multi-labels (scikit-learn)\n",
    "- **Tables √† cr√©er** : `annotation`, `emotion`, `annotation_emotion`, `entite_nommee`\n",
    "\n",
    "### üìä E3 - Production & Visualisation\n",
    "\n",
    "- **API REST** : FastAPI pour exposition des donn√©es\n",
    "- **Dashboard** : Power BI ou Streamlit pour visualisations interactives\n",
    "- **Orchestration** : Prefect/Airflow pour collecte automatique\n",
    "- **Monitoring** : Grafana + Prometheus pour m√©triques\n",
    "- **Tests** : pytest pour validation automatique\n",
    "- **Documentation** : API docs (Swagger/OpenAPI)\n",
    "\n",
    "### ‚úÖ E1 Valid√©\n",
    "\n",
    "- ‚úÖ Mod√©lisation Merise (MCD ‚Üí MLD ‚Üí MPD)\n",
    "- ‚úÖ 18 tables PostgreSQL cr√©√©es\n",
    "- ‚úÖ CRUD complet test√©\n",
    "- ‚úÖ 5 types de sources ing√©r√©es\n",
    "- ‚úÖ Tra√ßabilit√© (flux, manifests, versioning Git)\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ F√©licitations ! E1 est termin√© !**\n",
    "\n",
    "**Prochaines √©tapes** : Commencer E2 avec l'enrichissement IA\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
