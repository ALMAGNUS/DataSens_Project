{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üì• DataSens E1 ‚Äî Notebook 3 : Ingestion des 5 Sources\n",
        "\n",
        "**üéØ Objectif** : Ing√©rer r√©ellement les 5 types de sources avec tra√ßabilit√© compl√®te\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Plan d'ingestion\n",
        "\n",
        "1. **Fichier plat CSV** : Kaggle (50% ‚Üí Postgres, 50% ‚Üí raw)\n",
        "2. **Base de donn√©es** : Kaggle SQLite ‚Üí Postgres\n",
        "3. **API** : OpenWeatherMap ‚Üí meteo + flux\n",
        "4. **Web Scraping** : MonAvisCitoyen (dry-run) ‚Üí document\n",
        "5. **Big Data** : GDELT GKG ‚Üí evenement + document_evenement\n",
        "\n",
        "**Tra√ßabilit√©** : Manifest JSON par run avec chemins, compteurs, horodatages\n",
        "\n",
        "---\n",
        "\n",
        "## üîí RGPD & Gouvernance\n",
        "\n",
        "‚ö†Ô∏è **Rappel** : Pas de donn√©es personnelles directes (hash SHA-256), respect robots.txt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration et imports (architecture pipeline compl√®te)\n",
        "import os\n",
        "import hashlib\n",
        "import json\n",
        "import time\n",
        "import logging\n",
        "import traceback\n",
        "from datetime import datetime, timezone\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "from sqlalchemy import create_engine, text\n",
        "from dotenv import load_dotenv\n",
        "from minio import Minio\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Configuration\n",
        "NOTEBOOK_DIR = Path.cwd()\n",
        "PROJECT_ROOT = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == \"notebooks\" else NOTEBOOK_DIR\n",
        "load_dotenv(PROJECT_ROOT / \".env\")\n",
        "\n",
        "PG_HOST = os.getenv(\"POSTGRES_HOST\", \"localhost\")\n",
        "PG_PORT = int(os.getenv(\"POSTGRES_PORT\", \"5432\"))\n",
        "PG_DB = os.getenv(\"POSTGRES_DB\", \"datasens\")\n",
        "PG_USER = os.getenv(\"POSTGRES_USER\", \"ds_user\")\n",
        "PG_PASS = os.getenv(\"POSTGRES_PASS\", \"ds_pass\")\n",
        "\n",
        "PG_URL = f\"postgresql+psycopg2://{PG_USER}:{PG_PASS}@{PG_HOST}:{PG_PORT}/{PG_DB}\"\n",
        "engine = create_engine(PG_URL, future=True)\n",
        "\n",
        "# Configuration MinIO (DataLake)\n",
        "MINIO_ENDPOINT = os.getenv(\"MINIO_ENDPOINT\", \"http://localhost:9000\")\n",
        "MINIO_ACCESS_KEY = os.getenv(\"MINIO_ACCESS_KEY\", \"miniouser\")\n",
        "MINIO_SECRET_KEY = os.getenv(\"MINIO_SECRET_KEY\", \"miniosecret\")\n",
        "MINIO_BUCKET = os.getenv(\"MINIO_BUCKET\", \"datasens-raw\")\n",
        "\n",
        "RAW_DIR = PROJECT_ROOT / \"data\" / \"raw\"\n",
        "MANIFESTS_DIR = RAW_DIR / \"manifests\"\n",
        "LOGS_DIR = PROJECT_ROOT / \"logs\"\n",
        "\n",
        "# Cr√©er dossiers\n",
        "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
        "MANIFESTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "LOGS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# =====================================================\n",
        "# SYST√àME DE LOGGING (comme datasens_E1_v2.ipynb)\n",
        "# =====================================================\n",
        "log_timestamp = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
        "log_file = LOGS_DIR / f\"collecte_{log_timestamp}.log\"\n",
        "error_file = LOGS_DIR / f\"errors_{log_timestamp}.log\"\n",
        "\n",
        "logger = logging.getLogger(\"DataSens\")\n",
        "logger.setLevel(logging.DEBUG)\n",
        "\n",
        "file_formatter = logging.Formatter(\n",
        "    \"%(asctime)s | %(levelname)-8s | %(name)s | %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
        ")\n",
        "console_formatter = logging.Formatter(\n",
        "    \"[%(asctime)s] %(levelname)s - %(message)s\",\n",
        "    datefmt=\"%H:%M:%S\"\n",
        ")\n",
        "\n",
        "file_handler = logging.FileHandler(log_file, encoding=\"utf-8\")\n",
        "file_handler.setLevel(logging.INFO)\n",
        "file_handler.setFormatter(file_formatter)\n",
        "\n",
        "error_handler = logging.FileHandler(error_file, encoding=\"utf-8\")\n",
        "error_handler.setLevel(logging.ERROR)\n",
        "error_handler.setFormatter(file_formatter)\n",
        "\n",
        "console_handler = logging.StreamHandler()\n",
        "console_handler.setLevel(logging.INFO)\n",
        "console_handler.setFormatter(console_formatter)\n",
        "\n",
        "logger.addHandler(file_handler)\n",
        "logger.addHandler(error_handler)\n",
        "logger.addHandler(console_handler)\n",
        "\n",
        "def log_error(source: str, error: Exception, context: str = \"\"):\n",
        "    \"\"\"Log une erreur avec traceback complet\"\"\"\n",
        "    error_msg = f\"[{source}] {context}: {error!s}\"\n",
        "    logger.error(error_msg)\n",
        "    logger.error(f\"Traceback:\\n{traceback.format_exc()}\")\n",
        "\n",
        "logger.info(\"üöÄ Syst√®me de logging initialis√©\")\n",
        "logger.info(f\"üìÅ Logs: {log_file}\")\n",
        "logger.info(f\"‚ùå Erreurs: {error_file}\")\n",
        "\n",
        "# =====================================================\n",
        "# MINIO CLIENT (DataLake)\n",
        "# =====================================================\n",
        "try:\n",
        "    minio_client = Minio(\n",
        "        MINIO_ENDPOINT.replace(\"http://\", \"\").replace(\"https://\", \"\"),\n",
        "        access_key=MINIO_ACCESS_KEY,\n",
        "        secret_key=MINIO_SECRET_KEY,\n",
        "        secure=MINIO_ENDPOINT.startswith(\"https\")\n",
        "    )\n",
        "\n",
        "    def ensure_bucket(bucket: str = MINIO_BUCKET):\n",
        "        if not minio_client.bucket_exists(bucket):\n",
        "            minio_client.make_bucket(bucket)\n",
        "\n",
        "    def minio_upload(local_path: Path, dest_key: str) -> str:\n",
        "        \"\"\"Upload fichier vers MinIO DataLake\"\"\"\n",
        "        ensure_bucket(MINIO_BUCKET)\n",
        "        minio_client.fput_object(MINIO_BUCKET, dest_key, str(local_path))\n",
        "        return f\"s3://{MINIO_BUCKET}/{dest_key}\"\n",
        "\n",
        "    ensure_bucket()\n",
        "    logger.info(f\"‚úÖ MinIO OK ‚Üí bucket: {MINIO_BUCKET}\")\n",
        "except Exception as e:\n",
        "    logger.warning(f\"‚ö†Ô∏è MinIO non disponible: {e} - Mode local uniquement\")\n",
        "    minio_client = None\n",
        "    def minio_upload(local_path: Path, dest_key: str) -> str:\n",
        "        return f\"local://{local_path}\"\n",
        "\n",
        "# =====================================================\n",
        "# FONCTIONS UTILITAIRES\n",
        "# =====================================================\n",
        "def ts() -> str:\n",
        "    \"\"\"Timestamp UTC ISO compact\"\"\"\n",
        "    return datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
        "\n",
        "def sha256(s: str) -> str:\n",
        "    \"\"\"Hash SHA-256 pour d√©duplication\"\"\"\n",
        "    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "def get_source_id(conn, nom: str) -> int:\n",
        "    \"\"\"R√©cup√®re l'id_source depuis le nom\"\"\"\n",
        "    logger.info(f\"[get_source_id] Recherche source: {nom}\")\n",
        "    result = conn.execute(text(\"SELECT id_source FROM source WHERE nom = :nom\"), {\"nom\": nom}).fetchone()\n",
        "    if result:\n",
        "        logger.info(f\"   ‚Üí id_source trouv√©: {result[0]}\")\n",
        "        return result[0]\n",
        "    logger.warning(f\"   ‚Üí Source non trouv√©e: {nom}\")\n",
        "    return None\n",
        "\n",
        "def create_flux(conn, id_source: int, format_type: str = \"csv\", manifest_uri: str = None) -> int:\n",
        "    \"\"\"Cr√©e un flux et retourne id_flux\"\"\"\n",
        "    logger.info(f\"[create_flux] Cr√©ation flux pour id_source={id_source}, format={format_type}\")\n",
        "    result = conn.execute(text(\"\"\"\n",
        "        INSERT INTO flux (id_source, format, manifest_uri)\n",
        "        VALUES (:id_source, :format, :manifest_uri)\n",
        "        RETURNING id_flux\n",
        "    \"\"\"), {\"id_source\": id_source, \"format\": format_type, \"manifest_uri\": manifest_uri})\n",
        "    id_flux = result.scalar()\n",
        "    logger.info(f\"   ‚Üí id_flux cr√©√©: {id_flux}\")\n",
        "    return id_flux\n",
        "\n",
        "def ensure_territoire(conn, ville: str, code_insee: str = None, lat: float = None, lon: float = None) -> int:\n",
        "    \"\"\"Cr√©e ou r√©cup√®re un territoire\"\"\"\n",
        "    logger.info(f\"[ensure_territoire] V√©rification territoire: ville={ville}\")\n",
        "    result = conn.execute(text(\"SELECT id_territoire FROM territoire WHERE ville = :ville\"), {\"ville\": ville}).fetchone()\n",
        "    if result:\n",
        "        logger.info(f\"   ‚Üí id_territoire existant: {result[0]}\")\n",
        "        return result[0]\n",
        "    result = conn.execute(text(\"\"\"\n",
        "        INSERT INTO territoire (ville, code_insee, lat, lon)\n",
        "        VALUES (:ville, :code_insee, :lat, :lon)\n",
        "        RETURNING id_territoire\n",
        "    \"\"\"), {\"ville\": ville, \"code_insee\": code_insee, \"lat\": lat, \"lon\": lon})\n",
        "    id_territoire = result.scalar()\n",
        "    logger.info(f\"   ‚Üí id_territoire cr√©√©: {id_territoire}\")\n",
        "    return id_territoire\n",
        "\n",
        "def insert_documents(conn, docs: list) -> int:\n",
        "    \"\"\"Insertion batch de documents avec gestion doublons\"\"\"\n",
        "    logger.info(f\"[insert_documents] Insertion de {len(docs)} documents...\")\n",
        "    inserted = 0\n",
        "    for doc in docs:\n",
        "        try:\n",
        "            result = conn.execute(text(\"\"\"\n",
        "                INSERT INTO document (id_flux, id_territoire, titre, texte, langue, date_publication, hash_fingerprint)\n",
        "                VALUES (:id_flux, :id_territoire, :titre, :texte, :langue, :date_publication, :hash_fingerprint)\n",
        "                ON CONFLICT (hash_fingerprint) DO NOTHING\n",
        "                RETURNING id_doc\n",
        "            \"\"\"), doc)\n",
        "            id_doc = result.scalar()\n",
        "            if id_doc:\n",
        "                logger.info(f\"   ‚Üí Document ins√©r√©: id_doc={id_doc}, titre={doc.get('titre', '')[:40]}\")\n",
        "                inserted += 1\n",
        "        except Exception as e:\n",
        "            log_error(\"insert_documents\", e, f\"Erreur insertion document\")\n",
        "    logger.info(f\"   ‚Üí Total ins√©r√©s: {inserted}/{len(docs)}\")\n",
        "    return inserted\n",
        "\n",
        "print(\"‚úÖ Configuration pipeline charg√©e\")\n",
        "print(f\"   üìç PostgreSQL : {PG_HOST}:{PG_PORT}/{PG_DB}\")\n",
        "print(f\"   ‚òÅÔ∏è MinIO : {MINIO_BUCKET if minio_client else 'Mode local'}\")\n",
        "print(f\"   üìÇ Raw data : {RAW_DIR}\")\n",
        "print(f\"   üìÑ Logs : {LOGS_DIR}\")\n",
        "print(\"\\n‚úÖ Pipeline DataLake + PostgreSQL pr√™t !\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÑ Source 1/5 : Fichier plat CSV (Kaggle)\n",
        "\n",
        "**Architecture hybride (comme datasens_E1_v2.ipynb)** :\n",
        "- **50% ‚Üí PostgreSQL** : Donn√©es structur√©es pour requ√™tes SQL\n",
        "- **50% ‚Üí MinIO DataLake** : Donn√©es brutes pour analyses Big Data futures\n",
        "\n",
        "**Process** :\n",
        "1. Chargement CSV depuis `data/raw/kaggle/`\n",
        "2. Calcul SHA256 fingerprint pour d√©duplication\n",
        "3. Split al√©atoire 50/50\n",
        "4. Upload 50% vers MinIO (DataLake)\n",
        "5. Insertion 50% dans PostgreSQL avec tra√ßabilit√© (id_flux)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logger.info(\"üìÑ SOURCE 1/5 : Fichier plat CSV (Kaggle)\")\n",
        "logger.info(\"=\" * 80)\n",
        "\n",
        "# Rechercher fichier Kaggle existant ou cr√©er √©chantillon\n",
        "kaggle_csv_paths = [\n",
        "    RAW_DIR / \"kaggle\" / \"kaggle_sample.csv\",\n",
        "    PROJECT_ROOT / \"data\" / \"raw\" / \"kaggle\" / \"*.csv\",\n",
        "    Path.cwd() / \"data\" / \"raw\" / \"kaggle\" / \"*.csv\"\n",
        "]\n",
        "\n",
        "kaggle_csv_path = None\n",
        "for path in kaggle_csv_paths:\n",
        "    if path.exists():\n",
        "        kaggle_csv_path = path\n",
        "        break\n",
        "\n",
        "if not kaggle_csv_path or not kaggle_csv_path.exists():\n",
        "    logger.warning(\"‚ö†Ô∏è Fichier Kaggle non trouv√© ‚Äî Cr√©ation √©chantillon pour d√©mo\")\n",
        "    sample_data = pd.DataFrame({\n",
        "        \"text\": [\n",
        "            \"Great product, very satisfied!\",\n",
        "            \"Service terrible, avoid at all costs\",\n",
        "            \"Excellent quality, recommend\",\n",
        "            \"Bon produit, je recommande\",\n",
        "            \"Mauvais service, d√©√ßu\"\n",
        "        ],\n",
        "        \"langue\": [\"en\", \"en\", \"en\", \"fr\", \"fr\"],\n",
        "        \"date\": [datetime.now(timezone.utc)] * 5\n",
        "    })\n",
        "    kaggle_csv_path = RAW_DIR / \"kaggle\" / \"kaggle_sample.csv\"\n",
        "    kaggle_csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    sample_data.to_csv(kaggle_csv_path, index=False)\n",
        "    logger.info(f\"   ‚úÖ √âchantillon cr√©√© : {kaggle_csv_path.name}\")\n",
        "\n",
        "# Charger le CSV\n",
        "df_kaggle = pd.read_csv(kaggle_csv_path)\n",
        "logger.info(f\"üìä {len(df_kaggle)} lignes charg√©es\")\n",
        "\n",
        "# Split 50/50 (architecture hybride : PostgreSQL + MinIO)\n",
        "df_kaggle[\"hash_fingerprint\"] = df_kaggle[\"text\"].apply(lambda x: sha256(str(x)))\n",
        "mid_point = len(df_kaggle) // 2\n",
        "df_pg = df_kaggle.iloc[:mid_point].copy()  # 50% ‚Üí PostgreSQL\n",
        "df_raw = df_kaggle.iloc[mid_point:].copy()  # 50% ‚Üí MinIO DataLake\n",
        "\n",
        "logger.info(f\"   ‚Ä¢ 50% PostgreSQL : {len(df_pg)} lignes\")\n",
        "logger.info(f\"   ‚Ä¢ 50% MinIO DataLake : {len(df_raw)} lignes\")\n",
        "\n",
        "# Sauvegarder 50% en raw local + upload MinIO\n",
        "raw_output = RAW_DIR / \"kaggle\" / f\"kaggle_raw_{ts()}.csv\"\n",
        "df_raw.to_csv(raw_output, index=False)\n",
        "logger.info(f\"   ‚úÖ Sauvegard√© local : {raw_output.name}\")\n",
        "\n",
        "# Upload MinIO (50% bruts vers DataLake)\n",
        "try:\n",
        "    minio_uri = minio_upload(raw_output, f\"kaggle/{raw_output.name}\")\n",
        "    logger.info(f\"   ‚òÅÔ∏è Upload MinIO : {minio_uri}\")\n",
        "except Exception as e:\n",
        "    log_error(\"MinIO\", e, \"Upload fichier Kaggle\")\n",
        "    minio_uri = f\"local://{raw_output}\"\n",
        "\n",
        "# Ins√©rer 50% dans PostgreSQL\n",
        "with engine.begin() as conn:\n",
        "    id_source = get_source_id(conn, \"Kaggle CSV\")\n",
        "    if not id_source:\n",
        "        id_type = conn.execute(text(\"SELECT id_type_donnee FROM type_donnee WHERE libelle = 'Fichier plat'\")).scalar()\n",
        "        conn.execute(text(\"\"\"\n",
        "            INSERT INTO source (id_type_donnee, nom, url, fiabilite)\n",
        "            VALUES (:id_type, 'Kaggle CSV', 'https://www.kaggle.com', 0.8)\n",
        "        \"\"\"), {\"id_type\": id_type})\n",
        "        id_source = conn.execute(text(\"SELECT id_source FROM source WHERE nom = 'Kaggle CSV'\")).scalar()\n",
        "\n",
        "    id_flux = create_flux(conn, id_source, \"csv\", minio_uri)\n",
        "\n",
        "    # Pr√©parer documents pour insertion batch\n",
        "    docs = []\n",
        "    for _, row in df_pg.iterrows():\n",
        "        docs.append({\n",
        "            \"id_flux\": id_flux,\n",
        "            \"id_territoire\": None,\n",
        "            \"titre\": \"\",\n",
        "            \"texte\": str(row[\"text\"]),\n",
        "            \"langue\": row.get(\"langue\", \"en\"),\n",
        "            \"date_publication\": row.get(\"date\", datetime.now(timezone.utc)),\n",
        "            \"hash_fingerprint\": row[\"hash_fingerprint\"]\n",
        "        })\n",
        "\n",
        "    inserted = insert_documents(conn, docs)\n",
        "\n",
        "logger.info(f\"\\n‚úÖ Source 1/5 termin√©e : {inserted} docs PostgreSQL + {len(df_raw)} docs MinIO\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Architecture Pipeline (R√©f√©rence datasens_E1_v2.ipynb)\n",
        "\n",
        "**Ce notebook suit l'architecture du pipeline existant** :\n",
        "\n",
        "‚úÖ **Logging structur√©** : `logs/collecte_*.log` + `logs/errors_*.log`  \n",
        "‚úÖ **MinIO DataLake** : Upload automatique fichiers bruts ‚Üí `s3://datasens-raw/`  \n",
        "‚úÖ **PostgreSQL** : Insertion structur√©e avec tra√ßabilit√© (flux, manifests)  \n",
        "‚úÖ **Fonctions helpers** : `create_flux()`, `insert_documents()`, `ensure_territoire()`, `minio_upload()`  \n",
        "‚úÖ **D√©duplication** : Hash SHA-256 pour √©viter doublons  \n",
        "‚úÖ **RGPD** : Pas de donn√©es personnelles directes  \n",
        "\n",
        "**Sources 2-5** : Impl√©menter avec vraies API keys (voir `datasens_E1_v2.ipynb` pour exemples complets)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìã Cr√©ation du Manifest JSON\n",
        "\n",
        "G√©n√©ration d'un manifest JSON pour tra√ßabilit√© compl√®te de toutes les ingestions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# SOURCES 2, 3, 4, 5 : √Ä IMPL√âMENTER AVEC VRAIES SOURCES\n",
        "# =====================================================\n",
        "#\n",
        "# Pour respecter l'architecture pipeline du notebook datasens_E1_v2.ipynb,\n",
        "# les sources 2-5 doivent √™tre impl√©ment√©es avec :\n",
        "# 1. Collecte r√©elle depuis API/BDD/Scraping/GDELT\n",
        "# 2. Upload MinIO pour tra√ßabilit√© DataLake\n",
        "# 3. Insertion PostgreSQL avec fonctions helpers (create_flux, insert_documents)\n",
        "# 4. Logging complet via logger.info/error\n",
        "#\n",
        "# Voir notebook datasens_E1_v2.ipynb pour impl√©mentations compl√®tes :\n",
        "# - Source 2 : Kaggle DB (SQLite ‚Üí Postgres via Pandas)\n",
        "# - Source 3 : OpenWeatherMap API (voir Cell 20 du notebook existant)\n",
        "# - Source 4 : Web Scraping MonAvisCitoyen (voir Cell 26 du notebook existant)\n",
        "# - Source 5 : GDELT GKG Big Data (voir Cell 28 du notebook existant)\n",
        "\n",
        "logger.info(\"\\nüìã Pour sources 2-5 : Voir notebooks/datasens_E1_v2.ipynb\")\n",
        "logger.info(\"   ‚Üí Exemples complets avec vraies API keys et collectes r√©elles\")\n",
        "\n",
        "# =====================================================\n",
        "# MANIFEST JSON (Tra√ßabilit√© finale)\n",
        "# =====================================================\n",
        "logger.info(\"üìã Cr√©ation du manifest JSON\")\n",
        "logger.info(\"=\" * 80)\n",
        "\n",
        "# Compter les donn√©es collect√©es\n",
        "with engine.connect() as conn:\n",
        "    counts = {\n",
        "        \"documents\": conn.execute(text(\"SELECT COUNT(*) FROM document\")).scalar(),\n",
        "        \"flux\": conn.execute(text(\"SELECT COUNT(*) FROM flux\")).scalar(),\n",
        "        \"sources\": conn.execute(text(\"SELECT COUNT(*) FROM source\")).scalar(),\n",
        "        \"meteo\": conn.execute(text(\"SELECT COUNT(*) FROM meteo\")).scalar(),\n",
        "        \"evenements\": conn.execute(text(\"SELECT COUNT(*) FROM evenement\")).scalar(),\n",
        "    }\n",
        "\n",
        "manifest = {\n",
        "    \"run_id\": ts(),\n",
        "    \"timestamp_utc\": datetime.now(timezone.utc).isoformat(),\n",
        "    \"notebook_version\": \"03_ingest_sources.ipynb\",\n",
        "    \"sources_ingested\": [\n",
        "        \"Kaggle CSV (fichier plat - 50% PG + 50% MinIO)\",\n",
        "        \"Kaggle DB (base de donn√©es - √† impl√©menter)\",\n",
        "        \"OpenWeatherMap (API - √† impl√©menter)\",\n",
        "        \"MonAvisCitoyen (scraping - √† impl√©menter)\",\n",
        "        \"GDELT GKG (big data - √† impl√©menter)\"\n",
        "    ],\n",
        "    \"counts\": counts,\n",
        "    \"postgres_db\": PG_DB,\n",
        "    \"minio_bucket\": MINIO_BUCKET,\n",
        "    \"raw_data_location\": str(RAW_DIR),\n",
        "    \"log_file\": str(log_file)\n",
        "}\n",
        "\n",
        "# Sauvegarder manifest local + MinIO\n",
        "manifest_path = MANIFESTS_DIR / f\"manifest_{manifest['run_id']}.json\"\n",
        "manifest_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "with manifest_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(manifest, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "try:\n",
        "    manifest_minio_uri = minio_upload(manifest_path, f\"manifests/{manifest_path.name}\")\n",
        "    logger.info(f\"‚úÖ Manifest cr√©√© : {manifest_path.name}\")\n",
        "    logger.info(f\"‚òÅÔ∏è Manifest MinIO : {manifest_minio_uri}\")\n",
        "except Exception as e:\n",
        "    log_error(\"MinIO\", e, \"Upload manifest\")\n",
        "    manifest_minio_uri = f\"local://{manifest_path}\"\n",
        "\n",
        "logger.info(f\"\\nüìä R√©sum√© ingestion :\")\n",
        "for key, value in counts.items():\n",
        "    logger.info(f\"   ‚Ä¢ {key}: {value}\")\n",
        "\n",
        "logger.info(f\"\\n‚úÖ Ingestion termin√©e ! (Source 1/5 compl√®te, sources 2-5 √† documenter)\")\n",
        "logger.info(\"   ‚û°Ô∏è Passez au notebook 04_crud_tests.ipynb\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
