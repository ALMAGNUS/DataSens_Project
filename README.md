# ğŸ¯ DataSens - Projet E1 : Collecte Multi-Sources & DataLake

> **Projet acadÃ©mique E1** - Notebook Jupyter tout-en-un avec code inline simple

## ğŸ“Š Vue d'ensemble

DataSens est un **notebook Jupyter acadÃ©mique** dÃ©montrant la collecte de donnÃ©es depuis **5 types de sources diffÃ©rentes**, avec stockage hybride (MinIO + PostgreSQL) et traÃ§abilitÃ© complÃ¨te.

**ğŸ¯ Approche pÃ©dagogique** : Code simple et transparent dans le notebook, sans modules externes complexes.

### ğŸ“ Objectifs pÃ©dagogiques - 5 Sources
1. âœ… **Fichier plat** : Kaggle CSV 50% stockÃ© sur MinIO
2. âœ… **Base de donnÃ©es** : Kaggle 50% insÃ©rÃ© dans PostgreSQL (30k tweets)
3. âœ… **Web Scraping** : 6 sources citoyennes inline (Reddit, YouTube, SignalConso, Trustpilot, vie-publique.fr, data.gouv.fr)
4. âœ… **API** : 3 APIs inline (OpenWeatherMap, NewsAPI, RSS Multi-sources)
5. âœ… **Big Data** : GDELT GKG France (300 MB â†’ filtrage France)

**âœ¨ Points forts du projet** :
- **Code inline** : Tout dans le notebook, facile Ã  comprendre et debugger
- **Architecture DataLake** : MinIO (S3) + PostgreSQL (relationnel)
- **Gouvernance des donnÃ©es** : TraÃ§abilitÃ© complÃ¨te, dÃ©doublonnage SHA-256, RGPD
- **Orchestration Docker** : PostgreSQL, MinIO, Redis en containers
- **ReproductibilitÃ©** : `requirements.txt` + `.env` â†’ un seul notebook Ã  exÃ©cuter

---

## ğŸ—ï¸ Architecture

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  NOTEBOOK JUPYTER E1                         â”‚
â”‚              (Code inline simple et pÃ©dagogique)             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Cellule 1-10   : Setup (imports, MinIO, PostgreSQL)       â”‚
â”‚  Cellule 11-17  : Kaggle (CSV â†’ 50% MinIO + 50% PG)        â”‚
â”‚  Cellule 18-24  : PostgreSQL Kaggle (insertion 30k tweets) â”‚
â”‚  Cellule 25     : Web Scraping (9 sources inline)          â”‚
â”‚  Cellule 26     : APIs (NewsAPI, OpenWeather, RSS inline)  â”‚
â”‚  Cellule 27     : GDELT Big Data (filtrage France)         â”‚
â”‚  Cellule 28+    : Analyse & Visualisations                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              5 TYPES DE SOURCES (Exigence Projet)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. FICHIER PLAT     â†’ Kaggle 50% CSV (MinIO)              â”‚
â”‚  2. BASE DE DONNÃ‰ES  â†’ Kaggle 50% PostgreSQL (30k tweets)   â”‚
â”‚  3. WEB SCRAPING     â†’ 6 sources inline (Reddit, YouTube...)â”‚
â”‚  4. API              â†’ 3 sources inline (OWM, NewsAPI, RSS) â”‚
â”‚  5. BIG DATA         â†’ GDELT France (GKG filtrÃ© inline)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  COLLECTE INLINE (Code simple)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Code direct dans notebook (pas de .py externes)          â”‚
â”‚  â€¢ Try/except par source (robustesse)                       â”‚
â”‚  â€¢ Logs dÃ©taillÃ©s (debugging facile)                        â”‚
â”‚  â€¢ DÃ©doublonnage SHA-256                                    â”‚
â”‚  â€¢ Format unifiÃ© {titre, texte, source, url, date, langue}  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   DataLake (MinIO) â”‚   SGBD PostgreSQL (Merise)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Bruts 50% Kaggle â”‚ â€¢ 18 tables relationnelles             â”‚
â”‚ â€¢ Tous les CSV fluxâ”‚ â€¢ Type_donnee â†’ Source â†’ Flux â†’ Doc   â”‚
â”‚ â€¢ Versioning       â”‚ â€¢ Territoire, MÃ©tÃ©o, Indicateurs       â”‚
â”‚ â€¢ S3-compatible    â”‚ â€¢ TraÃ§abilitÃ© complÃ¨te                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Installation & DÃ©marrage

### PrÃ©requis
- Docker Desktop installÃ©
- Python 3.12+ avec Jupyter
- Git

### 1ï¸âƒ£ Cloner le projet
```bash
git clone https://github.com/votre-username/datasens-project.git
cd datasens-project
```

### 2ï¸âƒ£ Configuration

**CrÃ©er le fichier `.env`** avec tes API keys :

```bash
# PostgreSQL
POSTGRES_USER=ds_user
POSTGRES_PASSWORD=ds_pass
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=datasens

# MinIO
MINIO_ROOT_USER=admin
MINIO_ROOT_PASSWORD=admin123
MINIO_ENDPOINT=localhost:9000

# API Keys (optionnel pour dÃ©mo)
REDDIT_CLIENT_ID=ton_client_id
REDDIT_CLIENT_SECRET=ton_secret
YOUTUBE_API_KEY=ta_cle_youtube
NEWSAPI_KEY=ta_cle_newsapi
OWM_API_KEY=ta_cle_openweather
```

**ğŸ’¡ Astuce** : Les API keys sont optionnelles. Le notebook gÃ¨re gracieusement les erreurs (try/except) et continue avec les autres sources si une clÃ© manque.

### 3ï¸âƒ£ Lancer l'infrastructure
```bash
# DÃ©marrer MinIO, PostgreSQL, Redis
docker-compose up -d

# VÃ©rifier les services
docker-compose ps
```

### 4ï¸âƒ£ Installer les dÃ©pendances Python
```bash
# CrÃ©er l'environnement virtuel
python -m venv .venv

# Activer (Windows PowerShell)
.\.venv\Scripts\Activate.ps1

# Installer les packages
pip install -r requirements.txt
```

### 5ï¸âƒ£ ExÃ©cuter le notebook E1
```bash
cd notebooks
jupyter notebook datasens_E1_v2.ipynb
```

**ğŸ¯ Mode d'emploi** : ExÃ©cuter les cellules dans l'ordre (1-61). Chaque cellule est commentÃ©e et autonome.

**âš¡ Cellules clÃ©s** :
- **Cellule 8** : Configuration logging (fichiers de debug)
- **Cellule 25** : Web Scraping 6 sources (code inline simple)
- **Cellule 26** : APIs 3 sources (code inline simple)
- **Cellule 27** : GDELT Big Data France

---

## ğŸ“‹ Logs & Debugging

Le notebook gÃ©nÃ¨re des **fichiers de logs dÃ©taillÃ©s** pour tracer toutes les collectes et dÃ©boguer les erreurs :

### Fichiers gÃ©nÃ©rÃ©s (dossier `logs/`)

**ğŸ“„ `collecte_YYYYMMDD_HHMMSS.log`** - Log complet de la collecte :
```
2025-10-28 21:06:15 | INFO     | DataSens | ğŸš€ DÃ©marrage collecte Web Scraping Multi-Sources
2025-10-28 21:06:16 | INFO     | DataSens | [Reddit] Connexion API PRAW rÃ©ussie
2025-10-28 21:06:18 | INFO     | DataSens | [Reddit] r/france: 50 posts collectÃ©s
2025-10-28 21:06:19 | INFO     | DataSens | [Reddit] r/Paris: 50 posts collectÃ©s
2025-10-28 21:06:20 | INFO     | DataSens | [Reddit] âœ… Total: 100 posts
2025-10-28 21:06:21 | INFO     | DataSens | [YouTube] Connexion API Google v3 rÃ©ussie
2025-10-28 21:06:23 | INFO     | DataSens | [YouTube] âœ… 30 vidÃ©os collectÃ©es
2025-10-28 21:06:24 | WARNING  | DataSens | [SignalConso] 404 Client Error - API endpoint modifiÃ©
2025-10-28 21:06:24 | INFO     | DataSens | [SignalConso] âš ï¸ 0 signalements (skip)
2025-10-28 21:06:30 | INFO     | DataSens | [DataGouv] âœ… 7 datasets collectÃ©s
2025-10-28 21:06:35 | INFO     | DataSens | ğŸ“Š TOTAL: 86 documents collectÃ©s
2025-10-28 21:06:36 | INFO     | DataSens | âœ… Storage PostgreSQL + MinIO rÃ©ussi
```

**âŒ `errors_YYYYMMDD_HHMMSS.log`** - Erreurs uniquement avec traceback :
```
2025-10-28 21:06:24 | ERROR    | DataSens | [SignalConso] Collecte Ã©chouÃ©e: 404 Client Error: Not Found for url: https://signal.conso.gouv.fr/api/reports
2025-10-28 21:06:24 | ERROR    | DataSens | Traceback:
Traceback (most recent call last):
  File "<cell>", line 125, in <module>
    response.raise_for_status()
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://signal.conso.gouv.fr/api/reports?limit=100
```

### Comment consulter les logs

**Option 1 - PowerShell** :
```powershell
# Afficher le dernier log de collecte
Get-Content logs\collecte_*.log -Tail 50

# Afficher les erreurs uniquement
Get-Content logs\errors_*.log

# Suivre en temps rÃ©el (pendant exÃ©cution notebook)
Get-Content logs\collecte_*.log -Wait -Tail 20
```

**Option 2 - VS Code** :
- Ouvrir le dossier `logs/`
- Double-cliquer sur le fichier `.log`
- Recherche avec `Ctrl+F`

### Logs pour le prof

Les logs permettent de :
- âœ… Tracer **toutes les opÃ©rations** (timestamp prÃ©cis)
- âœ… Identifier **quelles sources fonctionnent**
- âœ… Voir **les erreurs avec traceback complet**
- âœ… DÃ©boguer **les problÃ¨mes d'API keys**
- âœ… Monitorer **le volume collectÃ© par source**

---## ğŸ” Approche Code Inline (Simple & PÃ©dagogique)

**Pourquoi code inline dans le notebook ?**

âœ… **SimplicitÃ©** : Pas de modules `.py` externes â†’ tout visible dans un seul fichier
âœ… **PÃ©dagogique** : Le jury voit chaque ligne de code, pas de "boÃ®te noire"
âœ… **Debugging facile** : Logs dÃ©taillÃ©s directement dans les cellules
âœ… **ReproductibilitÃ©** : `requirements.txt` + 1 notebook = tout fonctionne
âœ… **AcadÃ©mique** : Approche claire pour dÃ©monstration E1

**Exemple cellule 25 (Web Scraping)** :
```python
# Tout le code dans la cellule - pas d'import externe
import praw
reddit = praw.Reddit(client_id=os.getenv("REDDIT_CLIENT_ID"), ...)
for post in reddit.subreddit("france").hot(limit=50):
    all_data.append({"titre": post.title, "texte": post.selftext, ...})

# YouTube
from googleapiclient.discovery import build
youtube = build('youtube', 'v3', developerKey=os.getenv("YOUTUBE_API_KEY"))
response = youtube.search().list(q="france", maxResults=30).execute()
for video in response['items']:
    all_data.append({"titre": video['snippet']['title'], ...})

# ... 4 autres sources (SignalConso, Trustpilot, ViePublique, DataGouv)
```

---

## âš™ï¸ Stack Technique

### Infrastructure & DevOps
- **Docker** : Conteneurisation PostgreSQL, MinIO, Redis
- **Docker Compose** : Orchestration multi-conteneurs
- **Git** : Versioning (commits franÃ§ais, tags sÃ©mantiques)
- **GitHub** : HÃ©bergement code source
- **.env** : Gestion secrets (API keys sÃ©curisÃ©es)

### Base de DonnÃ©es & Storage
- **PostgreSQL 17** : SGBD relationnel (18 tables Merise E1)
- **MinIO** : Object Storage S3-compatible (DataLake)
- **SQLAlchemy 2.0** : ORM Python â†” PostgreSQL
- **psycopg2** : Driver PostgreSQL natif

### Data Processing
- **Python 3.13** : Langage principal
- **Pandas 2.3** : Manipulation DataFrames
- **Jupyter Notebook** : DÃ©veloppement interactif
- **NumPy 2.3** : Calculs numÃ©riques

### Data Collection (inline dans notebook)
- **PRAW 7.8** : Reddit API officielle
- **google-api-python-client 2.185** : YouTube Data API v3
- **requests 2.32** : HTTP client (SignalConso, Data.gouv, NewsAPI, OpenWeather)
- **BeautifulSoup4 4.14** : Web scraping (Trustpilot)
- **feedparser 6.0** : Parsing RSS/Atom (Vie Publique, multi-sources)

### Data Quality & Security
- **hashlib** : SHA-256 fingerprints (dÃ©doublonnage)
- **python-dotenv** : Chargement variables d'environnement
- **regex** : Validation et nettoyage donnÃ©es
- **datetime** : Gestion timestamps UTC

### Data Visualization
- **matplotlib 3.10** : Graphiques de base
- **seaborn 0.13** : Graphiques statistiques stylÃ©s

### APIs & Web Services
- **Kaggle API** : TÃ©lÃ©chargement datasets officiels
- **OpenWeatherMap API** : DonnÃ©es mÃ©tÃ©o temps rÃ©el
- **NewsAPI** : ActualitÃ©s internationales
- **Reddit API (PRAW)** : Posts subreddits franÃ§ais
- **YouTube Data API v3** : MÃ©tadonnÃ©es vidÃ©os
- **Signal.conso.gouv.fr API** : Signalements citoyens
- **data.gouv.fr API** : Open Data gouvernemental

### Big Data
- **GDELT Project** : Ã‰vÃ©nements mondiaux (GKG 300 MB â†’ 5-10 MB filtrÃ© France)
- **Filtrage** : V2Locations, V2Themes, V2Tone

---

## ğŸ“ Structure du projet

```
datasens-project/
â”œâ”€â”€ ğŸ““ notebooks/                    # Notebooks Jupyter
â”‚   â”œâ”€â”€ datasens_E1_v2.ipynb         # Version production (MinIO+PG)
â”‚   â””â”€â”€ datasens_E1_v3.ipynb         # Archive ancienne version
â”‚
â”œâ”€â”€ ğŸ³ docker-compose.yml            # Infrastructure Docker
â”œâ”€â”€ ğŸ“‹ requirements.txt              # DÃ©pendances Python
â”œâ”€â”€ ğŸ” .env                          # Configuration (secrets)
â”œâ”€â”€ ğŸ“„ .env.example                  # Template configuration
â”œâ”€â”€ ğŸš« .gitignore                    # Exclusions Git
â”‚
â”œâ”€â”€ ğŸ“‚ data/                         # DonnÃ©es collectÃ©es
â”‚   â”œâ”€â”€ raw/                         # Bruts (Kaggle, RSS, GDELT...)
â”‚   â”‚   â”œâ”€â”€ kaggle/                  # 60k tweets
â”‚   â”‚   â”œâ”€â”€ scraping/                # Multi-sources (Reddit, YouTube, etc.)
â”‚   â”‚   â”œâ”€â”€ rss/                     # Flux RSS multi-sources
â”‚   â”‚   â”œâ”€â”€ gdelt/                   # Big Data GKG France
â”‚   â”‚   â””â”€â”€ manifests/               # TraÃ§abilitÃ©
â”‚   â”œâ”€â”€ silver/                      # NettoyÃ©s (E2)
â”‚   â””â”€â”€ gold/                        # AgrÃ©gÃ©s (E2)
â”‚
â”œâ”€â”€ ğŸ“‚ datasens/                     # Code source
â”‚   â”œâ”€â”€ transformers/                # Nettoyage & enrichissement
â”‚   â”œâ”€â”€ loaders/                     # PostgreSQL & MinIO
â”‚   â”œâ”€â”€ utils/                       # Helpers
â”‚   â””â”€â”€ versions/                    # Snapshots PostgreSQL
â”‚
â”œâ”€â”€ ğŸ“‚ docs/                         # Documentation
â”‚   â””â”€â”€ GUIDE_TECHNIQUE_JURY.md      # Guide dÃ©taillÃ© pour le jury
â”‚
â”œâ”€â”€ ğŸ“‚ logs/                         # Logs de collecte
â””â”€â”€ ğŸ“‚ .github/workflows/            # CI/CD GitHub Actions
```

---

## ğŸ¨ ModÃ¨le de donnÃ©es (Merise E1)

### Tables principales - ChaÃ®ne de traÃ§abilitÃ©

```sql
TYPE_DONNEE (id, libelle, description)
    â†“
SOURCE (id, id_type_donnee, nom, url, fiabilite)
    â†“
FLUX (id, id_source, date_collecte, format, manifest_uri)
    â†“
DOCUMENT (id, id_flux, titre, texte, langue, hash_fingerprint)
```

**18 tables au total** : gÃ©ographie (territoire, mÃ©teo), indicateurs, thÃ¨mes, Ã©vÃ©nements, annotations

---

## ğŸ”’ SÃ©curitÃ© & RGPD

- âœ… **Pseudonymisation** : hash SHA-256 pour dÃ©doublonnage
- âœ… **Anonymisation auteurs** : aucune donnÃ©e personnelle stockÃ©e (RGPD)
- âœ… **Secrets sÃ©curisÃ©s** : `.env` exclu de Git (.gitignore)
- âœ… **APIs lÃ©gales** : Credentials officiels (Kaggle, NewsAPI, OWM, Reddit, YouTube)
- âœ… **Open Data gouvernemental** : Sources .gouv.fr (SignalConso, vie-publique.fr, data.gouv.fr)
- âœ… **Robots.txt** : respect des rÃ¨gles de scraping (Trustpilot vÃ©rification prÃ©alable)
- âœ… **Rate limiting** : pauses entre requÃªtes API
- âœ… **Aucune clÃ© payante** : 100% APIs gratuites

---

## ğŸ“Š RÃ©sultats E1

### âœ… 5 Types de sources respectÃ©s (100%)

#### 1ï¸âƒ£ FICHIER PLAT (Kaggle 50% CSV MinIO)
- **Format** : CSV brut stockÃ© sur MinIO
- **Volume** : 30,000 tweets (50% du dataset Kaggle)
- **Source** : Sentiment140 (EN) + French Twitter (FR)

#### 2ï¸âƒ£ BASE DE DONNÃ‰ES (Kaggle 50% PostgreSQL)
- **Format** : Insertion relationnelle PostgreSQL
- **Volume** : 30,000 tweets (50% restant)
- **Tables** : document, flux, territoire (Merise)

#### 3ï¸âƒ£ WEB SCRAPING (6 sources citoyennes lÃ©gales)

| Source | Collecteur | Tech | Description |
|--------|-----------|------|-------------|
| **Reddit France** | `reddit_collector.py` | PRAW API | r/france, r/Paris, r/Lyon |
| **YouTube** | `youtube_collector.py` | Google API | ChaÃ®nes officielles FR |
| **SignalConso** | `signalconso_collector.py` | Requests API | Signalements citoyens |
| **Trustpilot** | `trustpilot_collector.py` | BeautifulSoup4 | Avis consommateurs FR |
| **Vie Publique** | `vie_publique_collector.py` | Feedparser + BS4 | ActualitÃ©s gouv |
| **Data.gouv.fr** | `datagouv_collector.py` | Requests API | MÃ©tadonnÃ©es datasets |

**Total estimÃ©** : ~1,200 documents/jour

#### 4ï¸âƒ£ API (3 sources officielles)

| Source | Collecteur | Tech | Description |
|--------|-----------|------|-------------|
| **OpenWeatherMap** | `openweather_collector.py` | REST API | MÃ©tÃ©o 5 villes FR |
| **NewsAPI** | `newsapi_collector.py` | REST API | ActualitÃ©s internationales |
| **RSS Multi-sources** | `rss_collector.py` | Feedparser | Le Monde, BBC, France24, RFI |

**Total estimÃ©** : ~300 articles/jour

#### 5ï¸âƒ£ BIG DATA (GDELT GKG France)
- **Volume brut** : ~300 MB (fichier GKG derniÃ¨res 24h)
- **Filtrage France** : V2Locations contains "FR" â†’ ~5-10 MB
- **Ã‰vÃ©nements extraits** : ~500 Ã©vÃ©nements France
- **Analyses** : V2Tone (tonalitÃ© Ã©motionnelle), V2Themes (top 10 thÃ¨mes)

---

### ğŸ“ˆ MÃ©triques globales
- **Documents totaux** : ~62,400
- **Sources actives** : 5/5 types (100% conformitÃ© jury)
- **QualitÃ©** : 0 doublons (hash SHA-256), <5% nulls
- **TraÃ§abilitÃ©** : 100% (manifest par flux)
- **LÃ©galitÃ©** : 100% (APIs officielles + Open Data .gouv.fr)

---

## ğŸš€ CI/CD & DÃ©ploiement

### AccÃ¨s aux services

- **MinIO Console** : http://localhost:9001 (admin / admin123)
- **PostgreSQL** : localhost:5432 (ds_user / ds_pass)
- **Redis** : localhost:6379

### Commandes Docker

```bash
# DÃ©marrer
docker-compose up -d

# Logs
docker-compose logs -f

# ArrÃªter
docker-compose down

# Reset complet
docker-compose down -v
```

---

## ğŸ”® Roadmap E2/E3

### E2 - Enrichissement IA
- [ ] Analyse Ã©motionnelle (FlauBERT, CamemBERT)
- [ ] Extraction entitÃ©s nommÃ©es (spaCy)
- [ ] Embeddings vectoriels (sentence-transformers)
- [ ] Orchestration Prefect

### E3 - Production
- [ ] API REST (FastAPI)
- [ ] Dashboard Streamlit
- [ ] Monitoring Grafana/Prometheus
- [ ] Tests automatisÃ©s (pytest)

---

## ğŸ“ Licence

Projet acadÃ©mique - Usage Ã©ducatif uniquement
