# ğŸ¯ DataSens - Projet E1 : Collecte Multi-Sources & DataLake

[![GitHub release](https://img.shields.io/github/v/release/ALMAGNUS/DataSens_Project)](https://github.com/ALMAGNUS/DataSens_Project/releases)
[![GitHub stars](https://img.shields.io/github/stars/ALMAGNUS/DataSens_Project)](https://github.com/ALMAGNUS/DataSens_Project/stargazers)
[![GitHub issues](https://img.shields.io/github/issues/ALMAGNUS/DataSens_Project)](https://github.com/ALMAGNUS/DataSens_Project/issues)
[![Code size](https://img.shields.io/github/languages/code-size/ALMAGNUS/DataSens_Project)](https://github.com/ALMAGNUS/DataSens_Project)

> **Projet acadÃ©mique** - Architecture Big Data avec gouvernance des donnÃ©es

## ğŸ“Š Vue d'ensemble

DataSens est une plateforme de collecte, stockage et analyse de donnÃ©es hÃ©tÃ©rogÃ¨nes respectant les **5 types de sources exigÃ©es** pour le projet, avec traÃ§abilitÃ© complÃ¨te via un modÃ¨le Merise et infrastructure Big Data.

### ğŸ“ Objectifs pÃ©dagogiques - 5 Sources
1. âœ… **Fichier plat** : Kaggle CSV 50% stockÃ© sur MinIO
2. âœ… **Base de donnÃ©es** : Kaggle 50% insÃ©rÃ© dans PostgreSQL
3. âœ… **Web Scraping** : 6 sources citoyennes (Reddit, YouTube, SignalConso, Trustpilot, vie-publique.fr, data.gouv.fr)
4. âœ… **API** : 3 APIs (OpenWeatherMap, NewsAPI, RSS Multi-sources)
5. âœ… **Big Data** : GDELT GKG France (300 MB â†’ filtrage France)

**CompÃ©tences dÃ©montrÃ©es :**
- Architecture DataLake (MinIO) + SGBD (PostgreSQL)
- Gouvernance des donnÃ©es (traÃ§abilitÃ©, dÃ©doublonnage, RGPD)
- Orchestration Docker & CI/CD
- Notebooks reproductibles
- Respect des rÃ¨gles d'Ã©thique et lÃ©galitÃ© web scraping

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              5 TYPES DE SOURCES (Exigence Projet)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. FICHIER PLAT     â†’ Kaggle 50% CSV (MinIO)              â”‚
â”‚  2. BASE DE DONNÃ‰ES  â†’ Kaggle 50% PostgreSQL (30k tweets)   â”‚
â”‚  3. WEB SCRAPING     â†’ 6 sources citoyennes lÃ©gales         â”‚
â”‚  4. API              â†’ 3 APIs (OWM, NewsAPI, RSS)           â”‚
â”‚  5. BIG DATA         â†’ GDELT France (GKG filtrÃ©)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  COUCHE INGESTION (E1)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Collecte automatisÃ©e                                      â”‚
â”‚  â€¢ DÃ©doublonnage (hash fingerprint)                         â”‚
â”‚  â€¢ Validation qualitÃ©                                        â”‚
â”‚  â€¢ Manifest de traÃ§abilitÃ©                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   DataLake (MinIO) â”‚   SGBD PostgreSQL (Merise)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Bruts 50% Kaggle â”‚ â€¢ 18 tables relationnelles             â”‚
â”‚ â€¢ Tous les flux    â”‚ â€¢ Type_donnee â†’ Source â†’ Flux â†’ Doc   â”‚
â”‚ â€¢ Fichiers CSV/JSONâ”‚ â€¢ Territoire, MÃ©tÃ©o, Indicateurs      â”‚
â”‚ â€¢ Versioning       â”‚ â€¢ ThÃ¨mes, Ã‰vÃ©nements, Annotations     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Installation & DÃ©marrage

### PrÃ©requis
- Docker Desktop installÃ©
- Python 3.12+ avec Jupyter
- Git

### 1ï¸âƒ£ Cloner le projet
```bash
git clone https://github.com/votre-username/datasens-project.git
cd datasens-project
```

### 2ï¸âƒ£ Configuration
```bash
# Le fichier .env est dÃ©jÃ  configurÃ© avec les valeurs par dÃ©faut
# Ã‰diter les API keys si nÃ©cessaire : OWM_API_KEY, YOUTUBE_API_KEY, etc.
```

### 3ï¸âƒ£ Lancer l'infrastructure
```bash
# DÃ©marrer MinIO, PostgreSQL, Redis
docker-compose up -d

# VÃ©rifier les services
docker-compose ps
```

### 4ï¸âƒ£ Installer les dÃ©pendances Python
```bash
# CrÃ©er l'environnement virtuel
python -m venv .venv

# Activer (Windows PowerShell)
.\.venv\Scripts\Activate.ps1

# Installer les packages
pip install -r requirements.txt
```

### 5ï¸âƒ£ ExÃ©cuter le notebook E1
```bash
cd notebooks
jupyter notebook datasens_E1_v2.ipynb
```

ExÃ©cuter les cellules dans l'ordre (1-27)

---

## ğŸ“ Structure du projet

```
datasens-project/
â”œâ”€â”€ ï¿½ notebooks/                    # Notebooks Jupyter
â”‚   â”œâ”€â”€ datasens_E1_v2.ipynb         # Version production (MinIO+PG)
â”‚   â””â”€â”€ datasens_E1_v1.ipynb         # Version dÃ©mo (SQLite)
â”‚
â”œâ”€â”€ ğŸ³ docker-compose.yml            # Infrastructure Docker
â”œâ”€â”€ ğŸ“‹ requirements.txt              # DÃ©pendances Python
â”œâ”€â”€ ğŸ” .env                          # Configuration (secrets)
â”œâ”€â”€ ï¿½ .env.example                  # Template configuration
â”œâ”€â”€ ğŸš« .gitignore                    # Exclusions Git
â”‚
â”œâ”€â”€ ğŸ“‚ data/                         # DonnÃ©es collectÃ©es
â”‚   â”œâ”€â”€ raw/                         # Bruts (Kaggle, RSS, GDELT...)
â”‚   â”‚   â”œâ”€â”€ kaggle/                  # 60k tweets
â”‚   â”‚   â”œâ”€â”€ api/owm/                 # MÃ©tÃ©o 4 villes
â”‚   â”‚   â”œâ”€â”€ api/newsapi/             # 200 articles
â”‚   â”‚   â”œâ”€â”€ rss/                     # 77 articles multi-sources
â”‚   â”‚   â”œâ”€â”€ scraping/multi/          # Web scraping consolidÃ©
â”‚   â”‚   â”œâ”€â”€ scraping/viepublique/    # Consultations citoyennes
â”‚   â”‚   â”œâ”€â”€ scraping/datagouv/       # Budget participatif
â”‚   â”‚   â”œâ”€â”€ gdelt/                   # Big Data GKG France
â”‚   â”‚   â””â”€â”€ manifests/               # TraÃ§abilitÃ©
â”‚   â”œâ”€â”€ silver/                      # NettoyÃ©s (E2)
â”‚   â””â”€â”€ gold/                        # AgrÃ©gÃ©s (E2)
â”‚
â”œâ”€â”€ ğŸ“‚ docs/                         # Documentation
â”‚   â”œâ”€â”€ README.md                    # Cette documentation
â”‚   â”œâ”€â”€ DEPLOIEMENT_GITHUB.md        # Guide CI/CD
â”‚   â”œâ”€â”€ REDDIT_API_SETUP.md          # Config Reddit API
â”‚   â””â”€â”€ WEB_SCRAPING_GUIDE.md        # Ã‰thique scraping
â”‚
â”œâ”€â”€ ğŸ“‚ datasens/                     # MÃ©tadonnÃ©es
â”‚   â””â”€â”€ versions/                    # Snapshots PostgreSQL
â”‚
â”œâ”€â”€ ğŸ“‚ logs/                         # Logs de collecte
â””â”€â”€ ğŸ“‚ .github/workflows/            # CI/CD GitHub Actions
```

---

## ğŸ¨ ModÃ¨le de donnÃ©es (Merise E1)

### Tables principales - ChaÃ®ne de traÃ§abilitÃ©

```sql
TYPE_DONNEE (id, libelle, description)
    â†“
SOURCE (id, id_type_donnee, nom, url, fiabilite)
    â†“
FLUX (id, id_source, date_collecte, format, manifest_uri)
    â†“
DOCUMENT (id, id_flux, titre, texte, langue, hash_fingerprint)
```

**18 tables au total** : gÃ©ographie (territoire, mÃ©teo), indicateurs, thÃ¨mes, Ã©vÃ©nements, annotations

---

## ğŸ”’ SÃ©curitÃ© & RGPD

- âœ… **Pseudonymisation** : hash SHA-256 pour dÃ©doublonnage
- âœ… **Anonymisation auteurs** : aucune donnÃ©e personnelle stockÃ©e (RGPD)
- âœ… **Secrets sÃ©curisÃ©s** : `.env` exclu de Git (.gitignore)
- âœ… **APIs lÃ©gales** : Credentials officiels (Kaggle, NewsAPI, OWM, Reddit, YouTube)
- âœ… **Open Data gouvernemental** : Sources .gouv.fr (SignalConso, vie-publique.fr, data.gouv.fr)
- âœ… **Robots.txt** : respect des rÃ¨gles de scraping (Trustpilot vÃ©rification prÃ©alable)
- âœ… **Rate limiting** : pauses entre requÃªtes API
- âœ… **Aucune clÃ© payante** : 100% APIs gratuites

---

## ğŸ“Š RÃ©sultats E1

### âœ… 5 Types de sources respectÃ©s (100%)

#### 1ï¸âƒ£ FICHIER PLAT (Kaggle 50% CSV MinIO)
- **Format** : CSV brut stockÃ© sur MinIO
- **Volume** : 30,000 tweets (50% du dataset Kaggle)
- **Source** : Sentiment140 (EN) + French Twitter (FR)

#### 2ï¸âƒ£ BASE DE DONNÃ‰ES (Kaggle 50% PostgreSQL)
- **Format** : Insertion relationnelle PostgreSQL
- **Volume** : 30,000 tweets (50% restant)
- **Tables** : document, flux, territoire (Merise)

#### 3ï¸âƒ£ WEB SCRAPING (6 sources citoyennes lÃ©gales)
- **Reddit France** (API PRAW) : ~150 posts
- **YouTube Comments** (API) : ~300 commentaires texte
- **SignalConso** (Open Data gouv.fr) : ~500 signalements
- **Trustpilot FR** : ~100 avis
- **Vie-publique.fr** : ~50 consultations citoyennes
- **data.gouv.fr** : ~100 Budget Participatif
- **Total** : ~1,200 documents

#### 4ï¸âƒ£ API (3 APIs officielles)
- **OpenWeatherMap** : 4 relevÃ©s mÃ©tÃ©o (Paris, Lyon, Marseille, Lille)
- **NewsAPI** : ~200 articles (4 catÃ©gories FR)
- **RSS Multi-sources** : ~77 articles (Franceinfo + 20 Minutes + Le Monde)
- **Total** : ~280 documents

#### 5ï¸âƒ£ BIG DATA (GDELT GKG France)
- **Volume brut** : ~300 MB (fichier GKG derniÃ¨res 24h)
- **Filtrage France** : V2Locations contains "FR" â†’ ~5-10 MB
- **Ã‰vÃ©nements extraits** : ~500 Ã©vÃ©nements France
- **Analyses** : V2Tone (tonalitÃ© Ã©motionnelle), V2Themes (top 10 thÃ¨mes)

---

### ğŸ“ˆ MÃ©triques globales
- **Documents totaux** : ~62,400
- **Sources actives** : 5/5 types (100% conformitÃ© jury)
- **QualitÃ©** : 0 doublons (hash SHA-256), <5% nulls
- **TraÃ§abilitÃ©** : 100% (manifest par flux)
- **LÃ©galitÃ©** : 100% (APIs officielles + Open Data .gouv.fr)

---

## ğŸš€ CI/CD & DÃ©ploiement

### AccÃ¨s aux services

- **MinIO Console** : http://localhost:9001 (admin / admin123)
- **PostgreSQL** : localhost:5432 (ds_user / ds_pass)
- **Redis** : localhost:6379

### Commandes Docker

```bash
# DÃ©marrer
docker-compose up -d

# Logs
docker-compose logs -f

# ArrÃªter
docker-compose down

# Reset complet
docker-compose down -v
```

---

## ğŸ”® Roadmap E2/E3

### E2 - Enrichissement IA
- [ ] Analyse Ã©motionnelle (FlauBERT, CamemBERT)
- [ ] Extraction entitÃ©s nommÃ©es (spaCy)
- [ ] Embeddings vectoriels (sentence-transformers)
- [ ] Orchestration Prefect

### E3 - Production
- [ ] API REST (FastAPI)
- [ ] Dashboard Streamlit
- [ ] Monitoring Grafana/Prometheus
- [ ] Tests automatisÃ©s (pytest)

---

## ğŸ“ Licence

Projet acadÃ©mique - Usage Ã©ducatif uniquement
